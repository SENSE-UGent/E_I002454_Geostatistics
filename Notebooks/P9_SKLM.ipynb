{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was compiled for the course 'Geostatistics' at Ghent University (lecturer-in-charge: Prof. Dr. Ellen Van De Vijver; teaching assistant: Pablo De Weerdt). It consists of notebook snippets created by Michael Pyrcz. The code and markdown (text) snippets were edited specifically for this course, using the 'Jura data set' (Goovaerts, 1997) as example in the practical classes. Some new code snippets are also included to cover topics which were not found in the Geostastpy package demo books.\n",
    "\n",
    "This notebook is for educational purposes.<br> \n",
    "\n",
    "Guidelines for getting started were adapted from the 'Environmental Soil Sensing' course at Ghent University (lecturer-in-charge: Prof. Dr. Philippe De Smedt).<br> \n",
    "\n",
    "The Jura data set was taken from: Goovaerts P., 1997. Geostatistics for Natural Resources Evaluation. Oxford University Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't forget to save a copy on your Google drive before starting**\n",
    "\n",
    "You can also 'mount' your Google Drive in Google colab to directly access your Drive folders (e.g. to access data, previous notebooks etc.)\n",
    "\n",
    "Do not hesitate to contact us for questions or feel free to ask questions during the practical sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geostatistics: Introduction to geostatistical data analysis with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages for setup\n",
    "# -------------------------------------------- #\n",
    "\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clone the repository and add it to the path\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "\n",
    "    repo_path = '/content/E_I002454_Geostatistics'\n",
    "    if not os.path.exists(repo_path):\n",
    "        !git clone https://github.com/SENSE-UGent/E_I002454_Geostatistics.git\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.append(repo_path) #Default location in Google Colab after cloning\n",
    "\n",
    "else:\n",
    "    # if you are not using Google Colab, change the path to the location of the repository\n",
    "\n",
    "    repo_path = r'c:/Users/pdweerdt/Documents/Repos/E_I002454_Geostatistics' # Change this to the location of the repository on your machine\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.append(repo_path) \n",
    "\n",
    "# Import the setup function\n",
    "from Utils.setup import check_and_install_packages\n",
    "\n",
    "# Read the requirements.txt file\n",
    "\n",
    "requirements_path = repo_path + '/Utils/requirements.txt'\n",
    "\n",
    "with open(requirements_path) as f:\n",
    "    required_packages = f.read().splitlines()\n",
    "\n",
    "# Check and install packages\n",
    "check_and_install_packages(required_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geostatspy\n",
    "import geostatspy.GSLIB as GSLIB                              # GSLIB utilities, visualization and wrapper\n",
    "import geostatspy.geostats as geostats                        # if this raises an error, you might have to check your numba isntallation   \n",
    "print('GeostatsPy version: ' + str(geostatspy.__version__))   # these notebooks were tested with GeostatsPy version: 0.0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there was a small bug in the original kb2d_locations code from the geostatspy package\n",
    "# we have fixed this bug in the Utils.func module\n",
    "\n",
    "from Utils.func import kb2d_locations_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need some standard packages. These should have been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm                                         # suppress the status bar\n",
    "from functools import partialmethod\n",
    "\n",
    "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n",
    "                                   \n",
    "import numpy as np                                            # ndarrays for gridded data\n",
    "                                       \n",
    "import pandas as pd                                           # DataFrames for tabular data\n",
    "\n",
    "import matplotlib.pyplot as plt                               # for plotting\n",
    "\n",
    "from scipy import stats                                       # summary statistics\n",
    "\n",
    "plt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements\n",
    "\n",
    "ignore_warnings = True                                        # ignore warnings?\n",
    "if ignore_warnings == True:                                   \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.utils import io                                  # mute output from simulation\n",
    "\n",
    "seed = 42                                                     # random number seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are not required to run the given version of this practical exercise, but might be useful if you want to extend this notebook with more code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import math library\n",
    "import math\n",
    "\n",
    "import cmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr                              # Pearson product moment correlation\n",
    "from scipy.stats import spearmanr                             # spearman rank correlation    \n",
    "                                   \n",
    "import seaborn as sns                                         # advanced plotting\n",
    "\n",
    "import matplotlib as mpl                                        \n",
    "\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\n",
    "from matplotlib.colors import ListedColormap \n",
    "import matplotlib.ticker as mtick \n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Working Directory\n",
    "\n",
    "Do this to simplify subsequent reads and writes (avoid including the full address each time). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For use in Google Colab\n",
    "\n",
    "Run the following cell if you automatically want to get the data from the repository and store it on your Google Colab drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current directory and store it as a variable\n",
    "\n",
    "cd = os.getcwd()\n",
    "print('Current Working Directory is ', cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For local use\n",
    "\n",
    "Only run the following cell if you have the data locally stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the working directory, place an r in front to address special characters\n",
    "os.chdir(r'c:\\Users\\pdweerdt\\Documents\\Repos')\n",
    "\n",
    "# get the current directory and store it as a variable\n",
    "\n",
    "cd = os.getcwd()\n",
    "print('Current Working Directory is ', cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tabular & Gridded Data\n",
    "\n",
    "Here's the section to load our data file into a Pandas' DataFrame object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and visualize a grid also.\n",
    "\n",
    "Check the datatype of your gridded data.\n",
    "\n",
    "In this case it is actually also a .dat file, so we can use the same function to import it. The .grid extension was given to indicate that it is gridded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can adjust the relative Path to the data folder\n",
    "\n",
    "data_path = cd + '/E_I002454_Geostatistics/Hard_data' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '//prediction.dat'\n",
    "\n",
    "df = GSLIB.GSLIB2Dataframe(data_path + file_name) # read the data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_file_name = '//rocktype.grid'\n",
    "\n",
    "# load the data\n",
    "\n",
    "df_grid = GSLIB.GSLIB2Dataframe(data_path + grid_file_name)\n",
    "\n",
    "df_grid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Cd'\n",
    "unit = 'ppm'\n",
    "dist_unit = 'km'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define a colormap\n",
    "\n",
    "cmap = plt.cm.inferno                                         # color map inferno\n",
    "\n",
    "cmap_rainb = plt.cm.turbo # similar to what is shown on the slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Calculate local means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In P1 we calculated some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_feat = round((df[feature].values).min(), 2)                    # calculate the minimum\n",
    "max_feat = round((df[feature].values).max(), 2)                    # calculate the maximum\n",
    "mean_feat = round((df[feature].values).mean(), 2)                  # calculate the mean\n",
    "stdev_feat = round((df[feature].values).std(), 2)                  # calculate the standard deviation\n",
    "n_feat = df[feature].values.size                                   # calculate the number of data\n",
    "\n",
    "print('The minimum is ' + str(min_feat) + ' ' + str(unit) + '.')   # print univariate statistics\n",
    "print('The maximum is ' + str(max_feat) + ' ' + str(unit) + '.')\n",
    "print('The mean is ' + str(mean_feat) + ' ' + str(unit) + '.')\n",
    "print('The standard deviation is ' + str(stdev_feat) + ' ' + str(unit) + '.')\n",
    "print('The number of data is ' + str(n_feat) + '.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate local means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our exhaustive, categorical information (rocktype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_feature = 'rocktype' # feature of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the frequency of occurrence of each unique value in the grid\n",
    "df_grid[grid_feature].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make Custom Colorbar\n",
    "\n",
    "We make this colorbar to display our categorical data, in this case rock type or land use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_cat = plt.cm.get_cmap('Accent', 5) # make a colormap with 5 colors from Accent\n",
    "cmap_cat.set_over('white'); cmap_cat.set_under('white') # set the over and under value color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0; xmax = np.ceil(df.Xloc.max())                                   # range of x values\n",
    "ymin = 0; ymax = np.ceil(df.Yloc.max())                                   # range of y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSLIB.locmap_st(df_grid,'x', 'y', grid_feature,\n",
    "                0, 5.2, ymin, ymax, \n",
    "                1, 5, # set the value range for the color map\n",
    "                ('Location Map ' + str(grid_feature)), 'X (km)', 'Y (km)',\n",
    "             (str(grid_feature) + ' ()' + str(unit) + ')'), cmap_cat)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.8, wspace=0.1, hspace=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for df, we will calculate a local mean for each rocktype\n",
    "# check the rocktypes:\n",
    "df['Rock'].unique() # check the rocktypes\n",
    "print('The rocktypes are: ' + str(df['Rock'].unique())) # print the rocktypes\n",
    "print('The number of rocktypes is: ' + str(len(df['Rock'].unique()))) # print the number of rocktypes\n",
    "print('The number of data per rocktype is: ' + str(df.groupby('Rock').size())) # print the number of data per rocktype\n",
    "\n",
    "# step one, calculate the local mean for each rocktype\n",
    "for rock in df['Rock'].unique():\n",
    "    df_rock = df[df['Rock'] == rock] # select the data for the rocktype\n",
    "    mean_rock = df_rock[feature].mean() # calculate the mean for the rocktype\n",
    "    df.loc[df['Rock'] == rock, 'local_mean'] = mean_rock # assign the mean to the local_mean column\n",
    "    # round the mean to 3 decimals\n",
    "    df.loc[df['Rock'] == rock, 'local_mean'] = round(df.loc[df['Rock'] == rock, 'local_mean'], 3) # round the mean to 3 decimals\n",
    "\n",
    "# print the local mean for each rocktype\n",
    "for rock in df['Rock'].unique():\n",
    "    mean_rock = df[df['Rock'] == rock]['local_mean'].values[0] # get the mean for the rocktype\n",
    "    print('The local mean for ' + str(rock) + ' is ' + str(mean_rock) + ' ' + str(unit) + '.') # print the mean for the rocktype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculate residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate residuals by subtracting the local mean from the feature\n",
    "df['residual'] = df[feature] - df['local_mean'] # calculate the residuals\n",
    "# round the residuals to 3 decimal places\n",
    "df['residual'] = df['residual'].round(3) # round the residuals to 3 decimal places\n",
    "\n",
    "# print the distribution of the residuals\n",
    "print('The distribution of the residuals is:')\n",
    "print(df['residual'].describe()) # print the distribution of the residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the histogram of the residuals\n",
    "GSLIB.hist_st(df['residual'].values,\n",
    "              xmin = df['residual'].min(), xmax = df['residual'].max(),   # minimum and maximum feature values\n",
    "              log=False,cumul = False,bins=30,weights = None,\n",
    "           xlabel= 'residual' + ' (' + str(unit) + ')', title=str(feature) + 'residuals Data')\n",
    "# add_grid()\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Simple Kriging of residuals\n",
    "\n",
    "Remember that we use both our prediction data and a variogram model as inputs into the prediction algorithm. We already loaded our prediction data and grid. Let's have a look at the grid and also define our search neigbourhood and the variogram model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialise a new column into our grid dataframe for the OK prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grid[feature + 'SK'] = -99999 # assign a dummy value to the new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0; xmax = np.ceil(df.Xloc.max()) # range of x values\n",
    "ymin = 0; ymax = np.ceil(df.Yloc.max()) # range of y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSLIB.locmap_st(df_grid,'x', 'y', feature + 'SK',\n",
    "                0, 5.2, ymin, ymax, \n",
    "                1, 5, # set the value range for the color map\n",
    "                (\n",
    "                    'Location Map Grid points ' \n",
    "               #   + str(grid_feature)\n",
    "                 ), \n",
    "                 'X (km)', 'Y (km)',\n",
    "             (\n",
    "               #   str(grid_feature) + ' (' + str(unit) + ')'\n",
    "                 ), 'gray')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.8, wspace=0.1, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ignore the colobar in this case as we focus on the grid locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variogram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nug = 0.34; nst = 1 # 2 nest structure variogram model parameters\n",
    "it1 = 2;            # 1=spherical, 2=exponential, 3=gaussian\n",
    "cc1 = 0.455; \n",
    "azi1 = 45; \n",
    "hmaj1 = 0.8; hmin1 = 0.8\n",
    "\n",
    "if nst==2:\n",
    "\n",
    "    it2 = 2; # prefereably same as it1\n",
    "    cc2 = 4.2; # sill contribution of the second structure in major direction\n",
    "    azi2 = 45; # direction with maximum spatial continuity (perpendicular to the major axis)\n",
    "    hmaj2 = 1000; hmin2 = 1.1\n",
    "\n",
    "else:\n",
    "\n",
    "    it2= np.nan\n",
    "    cc2= np.nan\n",
    "    azi2= np.nan\n",
    "    hmaj2= np.nan\n",
    "    hmin2= np.nan\n",
    "\n",
    "vario_mod = GSLIB.make_variogram(nug,nst,\n",
    "                                it1,cc1,azi1,hmaj1,hmin1,\n",
    "                                it2,cc2,azi2,hmaj2,hmin2\n",
    "                                ) # make model object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_points = 15\n",
    "min_points = 2\n",
    "search_radii = [1, 1]   # search radius for neighbouring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try simple kriging\n",
    "\n",
    "* to switch to simple kriging set the kriging ktype to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display    \n",
    "\n",
    "tmin = -999; tmax = 9999\n",
    "\n",
    "ktype = 0   # 0=simple kriging; 1=ordinary kriging\n",
    "skmean = 0 # mean of the residuals\n",
    "SK_kmap, SK_vmap = kb2d_locations_v2(df,\"Xloc\", \"Yloc\",\"residual\",\n",
    "                                        tmin, tmax, \n",
    "                                        df_grid, 'x', 'y',\n",
    "                                        min_points, max_points, search_radii[0],\n",
    "                                        ktype, skmean, vario_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the OK_kmap to the df_grid\n",
    "df_grid[feature + 'SK_res'] = SK_kmap\n",
    "\n",
    "# add tjhe OK_vmap to the df_grid\n",
    "df_grid[feature + 'SK_res_var'] = SK_vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmap_rainb = plt.cm.turbo # similar to what is shown on the slides\n",
    "\n",
    "plt.subplot(121)\n",
    "GSLIB.locmap_st(df_grid,'x', 'y', feature + 'SK_res',\n",
    "                0, 5.2, ymin, ymax, \n",
    "                -1, 1, # set the value range for the color map\n",
    "                ('Location Map ' + str(feature + 'SK_res ')), 'X (km)', 'Y (km)',\n",
    "             (str(feature) + '_SK_res (' + str(unit) + ')'), cmap_rainb)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2)\n",
    "\n",
    "plt.subplot(122)\n",
    "GSLIB.locmap_st(df_grid,'x', 'y', feature + 'SK_res_var',\n",
    "                0, 5.2, ymin, ymax, \n",
    "                0, 1, # set the value range for the color map\n",
    "                ('Location Map ' + str(feature + ' SK_res_var')), 'X (km)', 'Y (km)',\n",
    "             (str(feature) + '_SK_res_var ()' + unit + '$^2$)' + ')'), cmap_rainb)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.3, hspace=0.3); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add local mean back to predicted residuals to obtain predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the local mean column to  the df_grid dataframe based on the rocktype column\n",
    "for rock in df['Rock'].unique():\n",
    "    mean_rock = df[df['Rock'] == rock]['local_mean'].values[0] # get the mean for the rocktype\n",
    "    df_grid.loc[df_grid['rocktype'] == rock, 'local_mean'] = mean_rock # assign the mean to the local_mean column\n",
    "\n",
    "df_grid[feature + 'SK'] = df_grid[feature + 'SK_res'] + df_grid['local_mean'] # add the local mean to the SK_residuals\n",
    "\n",
    "# Simple kriging with varying local means result\n",
    "df_grid[feature + 'SKLM'] = df_grid[feature + 'SK'].round(3) # round the SK result to 3 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the map\n",
    "GSLIB.locmap_st(df_grid,'x', 'y', feature + 'SKLM',\n",
    "                0, 5.2, ymin, ymax, \n",
    "                0, 3, # set the value range for the color map\n",
    "                ('Location Map ' + str(feature + ' SKLM')), 'X (km)', 'Y (km)',\n",
    "             (str(feature) + '_SKLM (' + str(unit) + ')'), cmap_rainb)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.8, wspace=0.1, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jackknife validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the process but choose validation locations as the grid where you want to make predictions...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '//validation.dat'\n",
    "\n",
    "df_val = GSLIB.GSLIB2Dataframe(data_path + file_name) # read the data\n",
    "\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display  \n",
    "\n",
    "val_method = 'jk'\n",
    "\n",
    "max_points = 15\n",
    "min_points = 2\n",
    "search_radii = [1,1]\n",
    "n_feat = df_val[feature].values.size # number of data in the validation set\n",
    "\n",
    "feature = 'Cd'\n",
    "\n",
    "# Initialize empty lists to add to the results df\n",
    "val_method_vals = []\n",
    "\n",
    "MPE_vals = []\n",
    "MSPE_vals = []\n",
    "RMSPE_vals = []\n",
    "MAPE_vals = []\n",
    "rel_nna_vals = []\n",
    "Pr_vals = []\n",
    "Sr_vals = []\n",
    "\n",
    "results_df_v = pd.DataFrame()\n",
    "\n",
    "# Perform validation initialize variables\n",
    "\n",
    "a_c = 0 # for the cumulative error\n",
    "a = 0 # for the error\n",
    "a_c_a = 0 #for the absolute cum error\n",
    "a_c_s = 0 #for the squared cum error\n",
    "\n",
    "data_pred = df.copy()\n",
    "data_val = df_val.copy()\n",
    "\n",
    "# Perform OK\n",
    "tmin = -999; tmax = 9999\n",
    "\n",
    "ktype = 0   # 0= simple kriging, 1= ordinary kriging\n",
    "skmean = 0\n",
    "SK_kmap, SK_vmap = kb2d_locations_v2(df,\"Xloc\", \"Yloc\",'residual',\n",
    "                                        tmin, tmax, \n",
    "                                        data_val, 'Xloc', 'Yloc',\n",
    "                                        min_points, max_points, search_radii[0],\n",
    "                                        ktype, skmean, \n",
    "                                        vario_mod # As modelled earlier!\n",
    "                                        )\n",
    "\n",
    "data_val['SK_res' + feature] = SK_kmap\n",
    "data_val['SK_res' + feature + '_var'] = SK_vmap\n",
    "\n",
    "# add the local mean column to  the data_val dataframe based on the rocktype column\n",
    "for rock in df['Rock'].unique():\n",
    "    mean_rock = df[df['Rock'] == rock]['local_mean'].values[0] # get the mean for the rocktype\n",
    "    data_val.loc[data_val['Rock'] == rock, 'local_mean'] = mean_rock # assign the mean to the local_mean column\n",
    "\n",
    "# calculate the SKLM prediction\n",
    "data_val[feature + 'SKLM'] = data_val['SK_res' + feature] + data_val['local_mean'] # add the local mean to the SK_residuals\n",
    "\n",
    "# Calculate error on test set\n",
    "data_val['r'] = data_val[feature + 'SKLM'] - data_val[feature] \n",
    "\n",
    "# print(\"r-value \", data_val['r'])\n",
    "\n",
    "data_val['r_s'] = data_val['r']**2\n",
    "\n",
    "data_val['r_a'] = data_val['r'].abs()\n",
    "\n",
    "# Calculate average error\n",
    "a = data_val['r'].mean()\n",
    "\n",
    "a_s = data_val['r_s'].mean()\n",
    "print(\"Mean Squared Error:\", a_s)\n",
    "\n",
    "a_a = data_val['r_a'].mean()\n",
    "print(\"Mean Absolute Error:\", a_a)\n",
    "\n",
    "a_c = data_val['r'].sum() #cumulative error\n",
    "\n",
    "a_c_a = data_val['r_a'].sum() #cumulative absolute error\n",
    "\n",
    "a_c_s = data_val['r_s'].sum() #cumulative squared error\n",
    "\n",
    "# Round ac and aca\n",
    "a_c = round(a_c, 2)\n",
    "a_c_a = round(a_c_a, 2)\n",
    "a_c_s = round(a_c_s, 2)\n",
    "\n",
    "#calculate Mean prediction error\n",
    "MPE = round(a_c/n_feat, 2)\n",
    "\n",
    "print(\"Mean Prediction Error:\", MPE)    \n",
    "\n",
    "#Calculate Mean squared prediction error\n",
    "MSPE = round(a_c_s/n_feat, 2)\n",
    "print(\"Mean Squared Prediction Error:\", MSPE)\n",
    "\n",
    "#Calculate Root mean squared prediction error\n",
    "RMSPE = round(math.sqrt(a_c_s/n_feat), 2)\n",
    "print(\"Root Mean Squared Prediction Error:\", RMSPE)\n",
    "\n",
    "#calculate Mean absolute prediction error\n",
    "MAPE = round(a_c_a/n_feat, 2)\n",
    "print(\"Mean Absolute Prediction Error:\", MAPE)\n",
    "\n",
    "#Pearson correlation coefficient\n",
    "#read in the data, drop na to avoid errors\n",
    "data_cor = data_val.dropna(subset=[feature, feature + 'SKLM'])\n",
    "\n",
    "#extract the columns of interest \n",
    "x = data_cor[feature]\n",
    "y = data_cor[feature + 'SKLM']\n",
    "\n",
    "#calculate the Pearson's correlation coefficient \n",
    "corr_p, _ = pearsonr(x, y)\n",
    "corr_p = round(corr_p, 2)\n",
    "print('Pearsons correlation: %.3f' % corr_p)\n",
    "\n",
    "# Spearman's Correlation:\n",
    "#calculate the Spearman's correlation coefficient \n",
    "corr_s, _ = spearmanr(x, y)\n",
    "corr_s = round(corr_s, 2)\n",
    "print('Spearmans correlation: %.3f' % corr_s)\n",
    "\n",
    "# Store the index values in the respective lists\n",
    "MPE_vals.append(MPE)\n",
    "MSPE_vals.append(MSPE)\n",
    "RMSPE_vals.append(RMSPE)\n",
    "MAPE_vals.append(MAPE)\n",
    "Pr_vals.append(corr_p)\n",
    "Sr_vals.append(corr_s)\n",
    "val_method_vals.append(val_method)\n",
    "\n",
    "# Create a new DataFrame to store the results for this variable and parameter settings\n",
    "results_temp_df = pd.DataFrame()\n",
    "results_temp_df['ValidationMethod'] = val_method_vals\n",
    "results_temp_df['MPE'] = MPE_vals\n",
    "results_temp_df['MSPE'] = MSPE_vals\n",
    "results_temp_df['RMSPE'] = RMSPE_vals\n",
    "results_temp_df['MAPE'] = MAPE_vals\n",
    "results_temp_df['PearsonCorr'] = Pr_vals\n",
    "results_temp_df['SpearmanCorr'] = Sr_vals\n",
    "\n",
    "# Append the results for this variable and parameter settings to the main DataFrame\n",
    "results_df_v_2d = pd.concat([results_df_v, results_temp_df], ignore_index=True)\n",
    "\n",
    "# results_df_v_2d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some extra code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot seperately the residuals for each rocktype\n",
    "\n",
    "# get number of rocktypes\n",
    "n_rocktypes = len(df['Rock'].unique()) # get the number of rocktypes\n",
    "\n",
    "\n",
    "for i, rock in enumerate(df['Rock'].unique()):\n",
    "    plt.subplot(5,1,i+1)\n",
    "    df_rock = df[df['Rock'] == rock] # select the data for the rocktype\n",
    "    # calculate the statistics of the residuals for each rocktype\n",
    "    print('rock ' + str(rock), df_rock['residual'].describe())\n",
    "\n",
    "    # plot the histogram of the residuals for the rocktype using\n",
    "    if i!=4:\n",
    "        GSLIB.hist_st(df_rock['residual'].values,\n",
    "              xmin = df_rock['residual'].min(), xmax = df_rock['residual'].max(),   # minimum and maximum feature values\n",
    "              log=False,cumul = False,bins=15,weights = None,\n",
    "           xlabel= None, title=str(feature) + ' Data rock' + ' ' + str(rock))\n",
    "    else:\n",
    "          GSLIB.hist_st(df_rock['residual'].values,\n",
    "              xmin = df_rock['residual'].min(), xmax = df_rock['residual'].max(),   # minimum and maximum feature values\n",
    "              log=False,cumul = False,bins=15,weights = None,\n",
    "           xlabel= 'residual' + ' (' + str(unit) + ')', title=str(feature) + ' Data rock' + ' ' + str(rock))  \n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=3.1, wspace=0.2, hspace=0.3); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geostatspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
