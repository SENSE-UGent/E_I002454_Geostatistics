{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was compiled for the course 'Geostatistics' at Ghent University (lecturer-in-charge: Prof. Dr. Ellen Van De Vijver; teaching assistant: Pablo De Weerdt). It consists of notebook snippets created by Michael Pyrcz. The code and markdown (text) snippets were edited specifically for this course, using the 'Jura data set' (Goovaerts, 1997) as example in the practical classes. Some new code snippets are also included to cover topics which were not found in the Geostastpy package demo books. In this case the inverse distance weighting and cross validation code was written by P. De Weerdt. <br> \n",
    "\n",
    "This notebook is for educational purposes.<br> \n",
    "\n",
    "Guidelines for getting started were adapted from the 'Environmental Soil Sensing' course at Ghent University (lecturer-in-charge: Prof. Dr. Philippe De Smedt).<br> \n",
    "\n",
    "The Jura data set was taken from: Goovaerts P., 1997. Geostatistics for Natural Resources Evaluation. Oxford University Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't forget to save a copy on your Google drive before starting**\n",
    "\n",
    "You can also 'mount' your Google Drive in Google colab to directly access your Drive folders (e.g. to access data, previous notebooks etc.)\n",
    "\n",
    "Do not hesitate to contact us for questions or feel free to ask questions during the practical sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geostatistics: Introduction to geostatistical data analysis with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages for setup\n",
    "# -------------------------------------------- #\n",
    "\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clone the repository and add it to the path\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "\n",
    "    repo_path = '/content/E_I002454_Geostatistics'\n",
    "    if not os.path.exists(repo_path):\n",
    "        !git clone https://github.com/SENSE-UGent/E_I002454_Geostatistics.git\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.append(repo_path) #Default location in Google Colab after cloning\n",
    "\n",
    "else:\n",
    "    # if you are not using Google Colab, change the path to the location of the repository\n",
    "\n",
    "    repo_path = r'c:/Users/pdweerdt/Documents/Repos/E_I002454_Geostatistics' # Change this to the location of the repository on your machine\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.append(repo_path) \n",
    "\n",
    "# Import the setup function\n",
    "from Utils.setup import check_and_install_packages\n",
    "\n",
    "# Read the requirements.txt file\n",
    "\n",
    "requirements_path = repo_path + '/Utils/requirements.txt'\n",
    "\n",
    "with open(requirements_path) as f:\n",
    "    required_packages = f.read().splitlines()\n",
    "\n",
    "# Check and install packages\n",
    "check_and_install_packages(required_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geostatspy.GSLIB as GSLIB                              # GSLIB utilities, visualization and wrapper\n",
    "import geostatspy as geostats\n",
    "print('GeostatsPy version: ' + str(geostats.__version__))   # these notebooks were tested with GeostatsPy version: 0.0.72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need some standard packages. These should have been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm                                         # suppress the status bar\n",
    "from functools import partialmethod\n",
    "\n",
    "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n",
    "                                   \n",
    "import numpy as np                                            # ndarrays for gridded data\n",
    "                                       \n",
    "import pandas as pd                                           # DataFrames for tabular data\n",
    "\n",
    "import matplotlib.pyplot as plt                               # for plotting\n",
    "\n",
    "from scipy import stats                                       # summary statistics\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import scipy.special\n",
    "\n",
    "plt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements\n",
    "\n",
    "ignore_warnings = True                                        # ignore warnings?\n",
    "if ignore_warnings == True:                                   \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.utils import io                                  # mute output from simulation\n",
    "\n",
    "seed = 42                                                     # random number seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.func import inverse_distance_weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are not required to run the given version of this practical exercise, but might be useful if you want to extend this notebook with more code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import math library\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr                              # Pearson product moment correlation\n",
    "from scipy.stats import spearmanr                             # spearman rank correlation    \n",
    "                                   \n",
    "import seaborn as sns                                         # advanced plotting\n",
    "\n",
    "import matplotlib as mpl                                        \n",
    "\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\n",
    "from matplotlib.colors import ListedColormap \n",
    "import matplotlib.ticker as mtick \n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Working Directory\n",
    "\n",
    "Do this to simplify subsequent reads and writes (avoid including the full address each time). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For use in Google Colab\n",
    "\n",
    "Run the following cell if you automatically want to get the data from the repository and store it on your Google Colab drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current directory and store it as a variable\n",
    "\n",
    "cd = os.getcwd()\n",
    "print('Current Working Directory is ', cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For local use\n",
    "\n",
    "Only run the following cell if you have the data locally stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the working directory, place an r in front to address special characters\n",
    "os.chdir(r'c:\\Users\\pdweerdt\\Documents\\Repos')\n",
    "\n",
    "# get the current directory and store it as a variable\n",
    "\n",
    "cd = os.getcwd()\n",
    "print('Current Working Directory is ', cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tabular & Gridded Data\n",
    "\n",
    "Here's the section to load our data file into a Pandas' DataFrame object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and visualize a grid also.\n",
    "\n",
    "Check the datatype of your gridded data.\n",
    "\n",
    "In this case it is actually also a .dat file, so we can use the same function to import it. The .grid extension was given to indicate that it is gridded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can adjust the relative Path to the data folder\n",
    "\n",
    "data_path = cd + '/E_I002454_Geostatistics/Hard_data' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '//prediction.dat'\n",
    "\n",
    "df = GSLIB.GSLIB2Dataframe(data_path + file_name) # read the data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_file_name = '//rocktype.grid'\n",
    "\n",
    "# load the data\n",
    "\n",
    "df_grid = GSLIB.GSLIB2Dataframe(data_path + grid_file_name)\n",
    "\n",
    "df_grid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grid.x.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Cd'\n",
    "unit = 'ppm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In P1 we calculated some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_feat = round((df[feature].values).min(), 2)                    # calculate the minimum\n",
    "max_feat = round((df[feature].values).max(), 2)                    # calculate the maximum\n",
    "mean_feat = round((df[feature].values).mean(), 2)                  # calculate the mean\n",
    "stdev_feat = round((df[feature].values).std(), 2)                  # calculate the standard deviation\n",
    "n_feat = df[feature].values.size                                   # calculate the number of data\n",
    "\n",
    "print('The minimum is ' + str(min_feat) + ' ' + str(unit) + '.')   # print univariate statistics\n",
    "print('The maximum is ' + str(max_feat) + ' ' + str(unit) + '.')\n",
    "print('The mean is ' + str(mean_feat) + ' ' + str(unit) + '.')\n",
    "print('The standard deviation is ' + str(stdev_feat) + ' ' + str(unit) + '.')\n",
    "print('The number of data is ' + str(n_feat) + '.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In P1 we already ran some code for plotting spatial data, this is always a good first step in spatial analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Spatial Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Colorbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the \n",
    "* [Matplotlib colormaps](https://matplotlib.org/stable/users/explain/colors/colormaps.html) for plotting with matplotlib\n",
    "* [seaborn color palettes](https://seaborn.pydata.org/generated/seaborn.color_palette.html) for plotting with seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define a colormap\n",
    "\n",
    "cmap = plt.cm.inferno                                         # color map inferno\n",
    "\n",
    "cmap_rainb = plt.cm.turbo # similar to what is shown on the slides\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the Area of Interest / Grid and Feature Limits\n",
    "\n",
    "Let's specify a reasonable extents for our grid and features:\n",
    "\n",
    "* we do this so we have consistent plots for comparison. \n",
    "\n",
    "* we design a grid that balances detail and computation time. Note kriging computation complexity scales\n",
    "\n",
    "* so if we half the cell size we have 4 times more grid cells in 2D, 4 times the runtime\n",
    "\n",
    "We could use commands like this one to find the minimum value of a feature:\n",
    "```python\n",
    "df[feature].min()\n",
    "```\n",
    "* But, it is natural to set the ranges manually. e.g. do you want your color bar to go from 0.05887 to 0.24230 exactly? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0; xmax = np.ceil(df.Xloc.max())                                   # range of x values\n",
    "ymin = 0; ymax = np.ceil(df.Yloc.max())                                   # range of y values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Tabular Data with Location Maps\n",
    "\n",
    "Let's try out locmap. This is a reimplementation of GSLIB's locmap program that uses matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSLIB.locmap_st(\n",
    "                df, 'Xloc', 'Yloc', feature, xmin, xmax, ymin, ymax, \n",
    "                min_feat, max_feat, # set the value range for the color map\n",
    "                ('Location Map ' + str(feature)),'X (km)','Y (km)',\n",
    "                (str(feature) + ' ()' + str(unit) + ')'), cmap\n",
    "                )\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Estimation Map\n",
    "\n",
    "Now we check the method by building maps with 3 different powers.\n",
    "\n",
    "* First we specify a grid and then rerun inverse distance for each map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have a look at the function first\n",
    "help(inverse_distance_weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a power of 2\n",
    "\n",
    "power = 2\n",
    "max_points = 15\n",
    "min_points = 2\n",
    "search_radii = [2, 2]\n",
    "\n",
    "# perform IDW\n",
    "inverse_distance_weighting(data=df, grid_points=df_grid, data_x_col='Xloc', data_y_col='Yloc', \n",
    "                             o_col=feature, grid_x_col='x', grid_y_col='y', \n",
    "                             power=power, max_points=max_points, min_points=min_points, search_radii=search_radii )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid visualisation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_feature = 'idw' + str(power) + feature\n",
    "\n",
    "GSLIB.locmap_st(df_grid,'x', 'y', grid_feature,\n",
    "                0, 5.2, ymin, ymax, \n",
    "                min_feat, max_feat, # set the value range for the color map\n",
    "                (\n",
    "                    'Location Map ' \n",
    "                 + str(grid_feature)\n",
    "                 ), \n",
    "                 'X (km)', 'Y (km)',\n",
    "             (\n",
    "                 str(grid_feature) + ' (' + str(unit) + ')'\n",
    "                 ), cmap)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Impact of the power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "# first set the power to a fixed value\n",
    "\n",
    "min_points = 2\n",
    "max_points = 15\n",
    "search_radii = [1, 1]\n",
    "\n",
    "# set the search radii that you want to compare\n",
    "power_imp = [2, 4]\n",
    "\n",
    "# set the search radii for the function, and iterate the idw function over the search radii\n",
    "for i, power in enumerate(power_imp):\n",
    "\n",
    "    # perform IDW\n",
    "    inverse_distance_weighting(data=df, grid_points=df_grid, data_x_col='Xloc', data_y_col='Yloc', \n",
    "                             o_col=feature, grid_x_col='x', grid_y_col='y', \n",
    "                             power=power, max_points=max_points, min_points=min_points, search_radii=search_radii)\n",
    "\n",
    "    grid_feature = 'idw' + str(power) + feature\n",
    "\n",
    "    # plot the results\n",
    "    plt.subplot(len(power_imp), 1, i + 1)\n",
    "    GSLIB.locmap_st(df_grid, 'x', 'y', grid_feature,\n",
    "                    0, 5.2, ymin, ymax,\n",
    "                    0, 3,  # set the value range for the color map\n",
    "                    (\n",
    "                        'Location Map '\n",
    "                        + str(grid_feature)\n",
    "                    ),\n",
    "                    'X (km)', 'Y (km)',\n",
    "                    (\n",
    "                        str(grid_feature) + ' (' + str(unit) + ')'\n",
    "                    ), cmap_rainb)\n",
    "plt.subplots_adjust(left=0.1, bottom=0.1, right=1.8, top=4, wspace=0.4, hspace=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "in this part we will perform a cross-validation to determine the optimal power parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "val_method = 'xv'\n",
    "\n",
    "max_points = 64\n",
    "min_points = 2\n",
    "search_radii = [1,1]\n",
    "\n",
    "feature = 'Cd'\n",
    "\n",
    "# We will compare validation results for different power values\n",
    "power_imp = [2, 4]\n",
    "\n",
    "# Initialize empty lists to add to the results df\n",
    "power_vals = []\n",
    "val_method_vals = []\n",
    "\n",
    "MPE_vals = []\n",
    "MSPE_vals = []\n",
    "RMSPE_vals = []\n",
    "MAPE_vals = []\n",
    "rel_nna_vals = []\n",
    "Pr_vals = []\n",
    "Sr_vals = []\n",
    "\n",
    "results_df_xv = pd.DataFrame()\n",
    "\n",
    "for power in power_imp:\n",
    "\n",
    "    # Perform leave-one-out cross validation, initialize variables\n",
    "\n",
    "    a_c = 0 # for the cumulative error\n",
    "    a = 0 # for the error\n",
    "    a_c_a = 0 #for the absolute cum error\n",
    "    a_c_s = 0 #for the squared cum error\n",
    "\n",
    "    #Initiate empty list to store results\n",
    "    p_list = []\n",
    "\n",
    "    # Split into prediction and validation sets by leaving one observation out in each iteration\n",
    "    for i in range(df.shape[0]):\n",
    "        data_pred = df.drop(i).reset_index(drop = True)\n",
    "        data_val = df.iloc[i].to_frame().T\n",
    "        data_val['idw' + str(power) + feature] = '' #function requires a 'p' column to append prediction results to\n",
    "        data_val['r'] = '' #add empty column for residuals\n",
    "\n",
    "        # Perform IDW\n",
    "        inverse_distance_weighting(data=data_pred, grid_points=data_val, data_x_col='Xloc', data_y_col='Yloc',\n",
    "                                    o_col=feature, grid_x_col='Xloc', grid_y_col='Yloc', power=power, max_points=max_points,\n",
    "                                    min_points=min_points, search_radii=search_radii)\n",
    "\n",
    "        p_list.extend(data_val['idw' + str(power) + feature].tolist())\n",
    "\n",
    "        # Calculate error on test set\n",
    "        data_val['r'] = data_val['idw' + str(power) + feature] - data_val[feature] \n",
    "\n",
    "        data_val['r_s'] = data_val['r']**2\n",
    "\n",
    "        data_val['r_a'] = data_val['r'].abs()\n",
    "\n",
    "        # Calculate average error\n",
    "        a = data_val['r'].mean()\n",
    "\n",
    "        a_s = data_val['r_s'].mean()\n",
    "\n",
    "        a_a = data_val['r_a'].mean()\n",
    "\n",
    "        if not pd.isna(a):\n",
    "\n",
    "            a_c += a #cumulative error\n",
    "\n",
    "            a_c_a += a_a #cumulative absolute error\n",
    "\n",
    "            a_c_s += a_s\n",
    "\n",
    "    # Check if the number of elements in the list is equal to the number of rows in the dataframe\n",
    "\n",
    "    if len(p_list) != df.shape[0]:\n",
    "        raise ValueError(\"The list and dataframe have different sizes\")\n",
    "        \n",
    "    else:\n",
    "        df['idw' + str(power) + feature + val_method] = p_list\n",
    "        print(len(p_list))\n",
    "\n",
    "    # Round ac and aca\n",
    "    a_c = round(a_c, 2)\n",
    "    a_c_a = round(a_c_a, 2)\n",
    "    a_c_s = round(a_c_s, 2)\n",
    "\n",
    "    n_feat = df.dropna(subset=[feature, 'idw' + str(power) + feature + val_method]).count()\n",
    "\n",
    "    #calculate Mean prediction error\n",
    "    MPE = round(a_c/n_feat, 2)\n",
    "\n",
    "    print(\"Mean Prediction Error:\", MPE)    \n",
    "\n",
    "    #Calculate Mean squared prediction error\n",
    "    MSPE = round(a_c_s/n_feat, 2)\n",
    "    print(\"Mean Squared Prediction Error:\", MSPE)\n",
    "\n",
    "    #Calculate Root mean squared prediction error\n",
    "    RMSPE = round(math.sqrt(a_c_s/n_feat), 2)\n",
    "    print(\"Root Mean Squared Prediction Error:\", RMSPE)\n",
    "\n",
    "    #calculate Mean absolute prediction error\n",
    "    MAPE = round(a_c_a/n_feat, 2)\n",
    "    print(\"Mean Absolute Prediction Error:\", MAPE)\n",
    "\n",
    "    #Pearson correlation coefficient\n",
    "    #read in the data, drop na to avoid errors\n",
    "    data_cor = df.dropna(subset=[feature, 'idw' + str(power) + feature + val_method])\n",
    "\n",
    "    #extract the columns of interest \n",
    "    x = data_cor[feature]\n",
    "    y = data_cor['idw' + str(power) + feature + val_method]\n",
    "\n",
    "    #calculate the Pearson's correlation coefficient \n",
    "    corr_p, _ = pearsonr(x, y)\n",
    "    corr_p = round(corr_p, 2)\n",
    "    print('Pearsons correlation: %.3f' % corr_p)\n",
    "\n",
    "    # Spearman's Correlation:\n",
    "    #calculate the Spearman's correlation coefficient \n",
    "    corr_s, _ = spearmanr(x, y)\n",
    "    corr_s = round(corr_s, 2)\n",
    "    print('Spearmans correlation: %.3f' % corr_s)\n",
    "\n",
    "    # Store the index values in the respective lists\n",
    "    MPE_vals.append(MPE)\n",
    "    MSPE_vals.append(MSPE)\n",
    "    RMSPE_vals.append(RMSPE)\n",
    "    MAPE_vals.append(MAPE)\n",
    "    Pr_vals.append(corr_p)\n",
    "    Sr_vals.append(corr_s)\n",
    "    power_vals.append(power)\n",
    "    val_method_vals.append(val_method)\n",
    "\n",
    "# Create a new DataFrame to store the results for this variable and parameter settings\n",
    "results_temp_df = pd.DataFrame()\n",
    "results_temp_df['idw_power'] = power_vals\n",
    "results_temp_df['ValidationMethod'] = val_method_vals\n",
    "results_temp_df['MPE'] = MPE_vals\n",
    "results_temp_df['MSPE'] = MSPE_vals\n",
    "results_temp_df['RMSPE'] = RMSPE_vals\n",
    "results_temp_df['MAPE'] = MAPE_vals\n",
    "results_temp_df['PearsonCorr'] = Pr_vals\n",
    "results_temp_df['SpearmanCorr'] = Sr_vals\n",
    "\n",
    "# Append the results for this variable and parameter settings to the main DataFrame\n",
    "results_df_xv_2d = pd.concat([results_df_xv, results_temp_df], ignore_index=True)\n",
    "\n",
    "results_df_xv_2d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss the validation indices..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Extra code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the Impact of Radius\n",
    "\n",
    "Radius is the maximum distance to look for neighbouring data\n",
    "\n",
    "* limited radius may result is image artifacts as data abruptly are excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "# first set the power to a fixed value\n",
    "power = 2\n",
    "min_points = 2\n",
    "max_points = 25\n",
    "\n",
    "# set the search radii that you want to compare\n",
    "search_radii_imp = [0.3, 0.4, 1]\n",
    "\n",
    "# set the search radii for the function, and iterate the idw function over the search radii\n",
    "for i, search_radius in enumerate(search_radii_imp):\n",
    "\n",
    "    search_radii = [search_radius, search_radius]\n",
    "\n",
    "    # perform IDW\n",
    "    inverse_distance_weighting(data=df, grid_points=df_grid, data_x_col='Xloc', data_y_col='Yloc', \n",
    "                             o_col=feature, grid_x_col='x', grid_y_col='y', \n",
    "                             power=power, max_points=max_points, min_points=min_points, search_radii=search_radii)\n",
    "\n",
    "    grid_feature = 'idw' + str(power) + feature\n",
    "\n",
    "    # plot the results\n",
    "    plt.subplot(len(search_radii_imp), 1, i + 1)\n",
    "    GSLIB.locmap_st(df_grid, 'x', 'y', grid_feature,\n",
    "                    0, 5.2, ymin, ymax,\n",
    "                    0, 3,  # set the value range for the color map\n",
    "                    (\n",
    "                        'Location Map '\n",
    "                        + str(grid_feature)\n",
    "                        + str(search_radius)\n",
    "                    ),\n",
    "                    'X (km)', 'Y (km)',\n",
    "                    (\n",
    "                        str(grid_feature) + ' (' + str(unit) + ')'\n",
    "                    ), cmap)\n",
    "plt.subplots_adjust(left=0.1, bottom=0.1, right=1.5, top=6, wspace=0.4, hspace=0.4)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geostatspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
