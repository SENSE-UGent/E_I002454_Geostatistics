{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was compiled for the course 'Geostatistics' at Ghent University. It consists primarily of notebook snippets created by Michael Pyrcz. The code and markdown (text) snippets were edited specifically for this course, using the Jura Data set as an example in the practical classes. Some new code snippets are also included to cover topics which were not found in the Geostastpy package demo books.<br> \n",
    "\n",
    "This notebook is for educational purposes.<br> \n",
    "\n",
    "Guidelines for getting started were adapted from ESS course (P. De Smedt).<br> \n",
    "\n",
    "The Jura dataset was taken from: Goovaerts P., 1997. Geostatistics for Natural Resources Evaluation. Oxford University Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't forget to save a copy on your Google drive before starting**\n",
    "\n",
    "You can also 'mount' your Google Drive in Google colab to directly access your Drive folders (e.g. to access data, previous notebooks etc.)\n",
    "\n",
    "Do not hesitate to contact us for questions or feel free to ask questions during the practical sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geostatistics: Introduction to geostatistical data analysis with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages for setup\n",
    "# -------------------------------------------- #\n",
    "\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clone the repository and add it to the path\n",
    "if 'google.colab' in sys.modules:\n",
    "    !git clone https://github.com/SENSE-UGent/E_I002454_Geostatistics.git\n",
    "    sys.path.append('/content/E_I002454_Geostatistics') #Default location in Google Colab after cloning\n",
    "else:\n",
    "    # if you are not using Google Colab, change the path to the location of the repository\n",
    "    sys.path.append(r'c:\\Users\\pdweerdt\\Documents\\Repos\\E_I002454_Geostatistics')\n",
    "    \n",
    "# Import the setup function\n",
    "from Utils.setup import check_and_install_packages\n",
    "\n",
    "# Read the requirements.txt file\n",
    "if 'google.colab' in sys.modules:\n",
    "    requirements_path = '/content/E_I002454_Geostatistics/Utils/requirements.txt'\n",
    "else:\n",
    "    requirements_path = 'c:/Users/pdweerdt/Documents/Repos/E_I002454_Geostatistics/Utils/requirements.txt'\n",
    "\n",
    "with open(requirements_path) as f:\n",
    "    required_packages = f.read().splitlines()\n",
    "\n",
    "# Check and install packages\n",
    "check_and_install_packages(required_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geostatspy.GSLIB as GSLIB                              # GSLIB utilities, visualization and wrapper\n",
    "import geostatspy\n",
    "print('GeostatsPy version: ' + str(geostatspy.__version__))   # these notebooks were tested with GeostatsPy version: 0.0.72"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need some standard packages. These should have been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm                                         # suppress the status bar\n",
    "from functools import partialmethod\n",
    "\n",
    "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n",
    "                                   \n",
    "import numpy as np                                            # ndarrays for gridded data\n",
    "                                       \n",
    "import pandas as pd                                           # DataFrames for tabular data\n",
    "\n",
    "import matplotlib.pyplot as plt                               # for plotting\n",
    "\n",
    "from scipy import stats                                       # summary statistics\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import scipy.special\n",
    "\n",
    "plt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements\n",
    "\n",
    "ignore_warnings = True                                        # ignore warnings?\n",
    "if ignore_warnings == True:                                   \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.utils import io                                  # mute output from simulation\n",
    "\n",
    "seed = 42                                                     # random number seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are not required but might be if you want to extend this notebook with more code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr                              # Pearson product moment correlation\n",
    "from scipy.stats import spearmanr                             # spearman rank correlation    \n",
    "                                   \n",
    "import seaborn as sns                                         # advanced plotting\n",
    "\n",
    "import matplotlib as mpl                                        \n",
    "\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\n",
    "from matplotlib.colors import ListedColormap \n",
    "import matplotlib.ticker as mtick \n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Working Directory\n",
    "\n",
    "Do this to simplify subsequent reads and writes (avoid including the full address each time). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For use in Google Colab\n",
    "\n",
    "Run the following cell if you automatically want to get the data from the repository and store it on your Google Colab drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the working directory to the cloned repository\n",
    "\n",
    "os.chdir('E_I002454_Geostatistics')\n",
    "\n",
    "# get the current directory and store it as a variable\n",
    "\n",
    "cd = os.getcwd()\n",
    "print(\"Current Working Directory is \" , cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For local use\n",
    "\n",
    "Only run the following cell if you have the data locally stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the working directory, place an r in front to address special characters\n",
    "os.chdir(r'C:\\\\Users\\\\pdweerdt\\\\OneDrive - UGent\\\\I002454 - Geostatistics\\\\AY 2024-2025\\\\Practicals')\n",
    "\n",
    "# get the current directory and store it as a variable\n",
    "\n",
    "cd = os.getcwd()\n",
    "print(\"Current Working Directory is \" , cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tabular & Gridded Data\n",
    "\n",
    "Here's the section to load our data file into a Pandas' DataFrame object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and visualize a grid also.\n",
    "\n",
    "Check the datatype of your gridded data.\n",
    "\n",
    "In this case it is actually also a .dat file, so we can use the same function to import it. The .grid extension was given to indicate that it is gridded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can adjust the relative Path to the data folder\n",
    "\n",
    "data_path = cd + '/Data/Hard_data' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '//prediction.dat'\n",
    "\n",
    "df = GSLIB.GSLIB2Dataframe(data_path + file_name) # read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_file_name = '//rocktype.grid'\n",
    "\n",
    "# load the data\n",
    "\n",
    "df_grid = GSLIB.GSLIB2Dataframe(data_path + grid_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Cd'\n",
    "unit = 'ppm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In P1 we calculated some statistics which we will use again in P2 to plot our data and do ANOVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = round((df[feature].values).min(),2)                    # calculate the minimum\n",
    "max = round((df[feature].values).max(),2)                    # calculate the maximum\n",
    "mean = round((df[feature].values).mean(),2)                  # calculate the mean\n",
    "stdev = round((df[feature].values).std(),2)                  # calculate the standard deviation\n",
    "n = df[feature].values.size                                  # calculate the number of data\n",
    "\n",
    "\n",
    "print('The minimum is ' + str(min) + ' ' + str(unit)+ '.') # print univariate statistics\n",
    "print('The maximum is ' + str(max) + ' ' + str(unit)+ '.')\n",
    "print('The mean is ' + str(mean) + ' ' + str(unit)+ '.')\n",
    "print('The standard deviation is ' + str(stdev) + ' ' + str(unit) + '.')\n",
    "print('The number of data is ' + str(n) + '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Spatial Data\n",
    "\n",
    "Michael J. Pyrcz, Professor, The University of Texas at Austin \n",
    "\n",
    "[Twitter](https:\\/twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Geostatistics Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [Applied Geostats in Python e-book](https://geostatsguy.github.io/GeostatsPyDemos_Book/intro.html) | [Applied Machine Learning in Python e-book](https://geostatsguy.github.io/MachineLearningDemos_Book/) | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n",
    "\n",
    "Chapter of e-book \"Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy\". \n",
    "\n",
    "Cite as: Pyrcz, M.J., 2024, Applied Geostatistics in Python: a Hands-on Guide with GeostatsPy, https://geostatsguy.github.io/GeostatsPyDemos_Book. \n",
    "\n",
    "By Michael J. Pyrcz <br />\n",
    "&copy; Copyright 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tutorial for / demonstration of **Visualizing Spatial Data** with GeostatsPy, including,\n",
    "\n",
    "* **location maps** for plotting tabular spatial data, data points in space with one or more features\n",
    "* **pixel plots** for plotting gridded, exhaustive spatial data and models with one or more features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Colorbar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the \n",
    "* [Matplotlib colormaps](https://matplotlib.org/stable/users/explain/colors/colormaps.html) for plotting with matplotlib\n",
    "* [seaborn color palettes](https://seaborn.pydata.org/generated/seaborn.color_palette.html) for plotting with seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.inferno                                         # color map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specify the Area of Interest / Grid and Feature Limits\n",
    "\n",
    "Let's specify a reasonable extents for our grid and features:\n",
    "\n",
    "* we do this so we have consistent plots for comparison. \n",
    "\n",
    "* we design a grid that balances detail and computation time. Note kriging computation complexity scales\n",
    "\n",
    "* so if we half the cell size we have 4 times more grid cells in 2D, 4 times the runtime\n",
    "\n",
    "We could use commands like this one to find the minimum value of a feature:\n",
    "```python\n",
    "df[feature].min()\n",
    "```\n",
    "* But, it is natural to set the ranges manually. e.g. do you want your color bar to go from 0.05887 to 0.24230 exactly? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0; xmax = np.ceil(df.Xloc.max())                                   # range of x values\n",
    "ymin = 0; ymax = np.ceil(df.Yloc.max())                                   # range of y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the dimensions of the grid\n",
    "df_grid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Tabular Data with Location Maps\n",
    "\n",
    "Let's try out locmap. This is a reimplementation of GSLIB's locmap program that uses matplotlib. I hope you find it simpler than matplotlib, if you want to get more advanced and build custom plots lock at the source. If you improve it, send me the new code. Any help is appreciated. To see the parameters, just type the command name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the plotting parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSLIB.locmap_st(\n",
    "                df, 'Xloc', 'Yloc', feature, xmin, xmax, ymin, ymax, \n",
    "                min, max, # set the value range for the color map\n",
    "                ('Location Map ' + str(feature)),'X (km)','Y (km)',\n",
    "                (str(feature) + ' ()' + str(unit) + ')'), cmap\n",
    "                )\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialise a new column into our grid dataframe for the trend surface data. We start with a 1st degree polynomial trend surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 1\n",
    "\n",
    "df_grid[feature + 'poly' + str(degree)] = -99999 # assign a dummy value to the new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSLIB.locmap_st(df_grid,'x', 'y', feature + 'pol' + str(degree),\n",
    "                0, 5.2, ymin, ymax, \n",
    "                1, 5, # set the value range for the color map\n",
    "                (\n",
    "                    'Location Map ' \n",
    "               #   + str(grid_feature)\n",
    "                 ), \n",
    "                 'X (km)', 'Y (km)',\n",
    "             (\n",
    "               #   str(grid_feature) + ' (' + str(unit) + ')'\n",
    "                 ), 'gray')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can hardly differentiate the individual grid points but they are surely there! Also ignore the colobar in this case as we focus on the grid locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend surfaces: Polynomial models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code written for Geostatistics course at Ghent University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define x, y, and feature arrays for the trend surface fitting\n",
    "\n",
    "coords = df[['Xloc','Yloc']].values\n",
    "feature_values = df[feature].values\n",
    "\n",
    "grid_coords = df_grid[['x','y']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our polynomial model, with whatever degree we want\n",
    "degree=1\n",
    "\n",
    "# PolynomialFeatures will create a new matrix consisting of all polynomial combinations \n",
    "# of the features with a degree less than or equal to the degree we just gave the model (2)\n",
    "poly_model = PolynomialFeatures(degree=degree)\n",
    "\n",
    "# transform out polynomial features\n",
    "poly_coords = poly_model.fit_transform(coords)\n",
    "\n",
    "# should be in the form [1, a, b, a^2, ab, b^2], this is without using the feature values!!\n",
    "print(f'initial values {coords[0]}\\nMapped to {poly_coords[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's fit the model, with the feature values\n",
    "poly_model.fit(coords, feature_values)\n",
    "\n",
    "# we use linear regression as a base!!! ** sometimes misunderstood **\n",
    "regression_model = LinearRegression()\n",
    "\n",
    "regression_model.fit(poly_coords, feature_values)\n",
    "\n",
    "y_pred = regression_model.predict(poly_coords)\n",
    "\n",
    "print(regression_model.intercept_)\n",
    "print(regression_model.coef_[1:]) # skip the first term, which is 0\n",
    "\n",
    "# Construct the polynomial equation\n",
    "terms = poly_model.get_feature_names_out(['x', 'y'])\n",
    "equation = f\"{regression_model.intercept_:.3f} + \" + \" + \".join(\n",
    "    f\"{coef:.3f}*{term}\" for coef, term in zip(regression_model.coef_[1:], terms[1:]) # skip the first term, which is 0\n",
    ")\n",
    "print(f\"Polynomial Equation: {equation}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This polynomial model can now be fitted onto our grid, and we can compare different polynomial degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid fitting & plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid fitting\n",
    "\n",
    "number_degrees = [1,2,3]\n",
    "\n",
    "for degree in number_degrees:\n",
    "\n",
    "      # our feature\n",
    "      poly_model = PolynomialFeatures(degree=degree)\n",
    "\n",
    "      poly_coords = poly_model.fit_transform(coords)\n",
    "\n",
    "      regression_model = LinearRegression()\n",
    "      regression_model.fit(poly_coords, feature_values)\n",
    "\n",
    "      poly_grid_coords = poly_model.fit_transform(grid_coords)\n",
    "      df_grid[str(feature) + 'poly' + str(degree)] = regression_model.predict(poly_grid_coords)\n",
    "\n",
    "      print(f\"Degree {degree} Polynomial:\")\n",
    "      print(f\"Intercept: {regression_model.intercept_}\")\n",
    "      print(f\"Coefficients: {regression_model.coef_[1:]}\") # skip the first term, which is 0\n",
    "\n",
    "      # Construct the polynomial equation\n",
    "      terms = poly_model.get_feature_names_out(['x', 'y'])\n",
    "      equation = f\"{regression_model.intercept_:.3f} + \" + \" + \".join(\n",
    "            f\"{coef:.3f}*{term}\" for coef, term in zip(regression_model.coef_[1:], terms[1:]) # skip the first term, which is 0\n",
    "      )\n",
    "      print(f\"Polynomial Equation: {equation}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = 1  # set the degree of the polynomial\n",
    "\n",
    "GSLIB.locmap_st(df_grid,'x', 'y', str(feature) + 'poly' + str(degree),\n",
    "                0, 5.2, \n",
    "                ymin, ymax, \n",
    "                min, 2, # set the value range for the color map\n",
    "                ('Location Map ' + str(str(feature) + 'poly' + str(degree))), 'X (km)', 'Y (km)',\n",
    "             (str(str(feature) + 'poly' + str(degree)) + ' ()' + str(unit) + ')'), cmap)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting dataframes, figures can also be exported to your local Geostatistics folder (e.g. on your C: drive). These files can be opened in other software for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define the path where to export\n",
    "export_path = 'C:\\\\Users\\\\pdweerdt\\\\OneDrive - UGent\\\\I002454 - Geostatistics\\\\AY 2024-2025\\\\Practicals\\\\Data\\\\Export'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  export the grid to a csv file\n",
    "df_grid.to_csv(export_path + '\\\\grid_trends.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### GSLIB format (.dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required format if you want to open your results in SGeMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the grid to a GSLIB file\n",
    "GSLIB.Dataframe2GSLIB(export_path + \n",
    "                      '\\\\grid_trends.dat', #optional: change the name of the file here\n",
    "                      df_grid )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will run the same fitting to get the results specifically at the locations of our observations (remember that this method is non-exact, so we will get different values from our trend fitting results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid prediction\n",
    "\n",
    "number_degrees = [1,2,3]\n",
    "\n",
    "for degree in number_degrees:\n",
    "\n",
    "      # for our feature\n",
    "      poly_model = PolynomialFeatures(degree=degree)\n",
    "\n",
    "      poly_x_coords = poly_model.fit_transform(coords)\n",
    "\n",
    "      regression_model = LinearRegression()\n",
    "      regression_model.fit(poly_x_coords, feature_values)\n",
    "\n",
    "      poly_grid_coords = poly_model.fit_transform(coords)\n",
    "\n",
    "      df[str(feature) + 'poly' + str(degree)] = regression_model.predict(poly_grid_coords)\n",
    "\n",
    "      print(f\"Degree {degree} Polynomial:\")\n",
    "      print(f\"Intercept: {regression_model.intercept_}\")\n",
    "      print(f\"Coefficients: {regression_model.coef_[1:]}\")\n",
    "\n",
    "      # Construct the polynomial equation\n",
    "      terms = poly_model.get_feature_names_out(['x', 'y'])\n",
    "      equation = f\"{regression_model.intercept_:.3f} + \" + \" + \".join(\n",
    "            f\"{coef:.3f}*{term}\" for coef, term in zip(regression_model.coef_[1:], terms[1:]) # skip the first term, which is 0\n",
    "      )\n",
    "      print(f\"Polynomial Equation: {equation}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA\n",
    "degrees = [1, 2, 3]\n",
    "\n",
    "anova_results = []\n",
    "\n",
    "for degree in degrees:\n",
    "    print(f\"{feature} poly {degree} ANOVA\")\n",
    "    print(f\"polynomial {degree}\")\n",
    "\n",
    "    # Create ANOVA backbone table\n",
    "    data = [['Trend', '', '', '', '', '', '', ''], ['Deviation', '', '', '', '', '', '', ''], ['Total', '', '', '', '', '', '', '']]\n",
    "    anova_table = pd.DataFrame(data, columns=['Source of Variation', 'SS', 'df', 'MS', 'F', 'P-value', 'F crit', 'R_sq (%)'])\n",
    "    anova_table.set_index('Source of Variation', inplace=True)\n",
    "\n",
    "    n_data = df.shape[0]\n",
    "\n",
    "    mean = df[feature].mean()\n",
    "\n",
    "    n_variables = 2\n",
    "    n_terms = scipy.special.binom(n_variables + degree, degree)  # includes intercept\n",
    "\n",
    "    # Calculate SSTR and update ANOVA table\n",
    "    SSr = sum((df[f\"{feature}poly{degree}\"] - mean) ** 2)\n",
    "    anova_table.at['Trend', 'SS'] = SSr\n",
    "\n",
    "    # Calculate SSTR and update ANOVA table\n",
    "    SSt = sum((df[feature] - mean) ** 2)\n",
    "    anova_table.at['Total', 'SS'] = SSt\n",
    "\n",
    "    # Calculate SSE and update ANOVA table\n",
    "    SSd = SSt - SSr\n",
    "    anova_table.at['Deviation', 'SS'] = SSd\n",
    "\n",
    "    # Update degree of freedom\n",
    "    anova_table.at['Trend', 'df'] = n_terms - 1\n",
    "    anova_table.at['Deviation', 'df'] = n_data - n_terms\n",
    "    anova_table.at['Total', 'df'] = n_data - 1\n",
    "\n",
    "    # Calculate MS\n",
    "    anova_table['MS'] = anova_table['SS'] / anova_table['df']\n",
    "\n",
    "    # Calculate F\n",
    "    F = anova_table.at['Trend', 'MS'] / anova_table.at['Deviation', 'MS']\n",
    "    anova_table.at['Trend', 'F'] = F\n",
    "\n",
    "    # P-value\n",
    "    anova_table.at['Trend', 'P-value'] = 1 - stats.f.cdf(F, anova_table.at['Trend', 'df'], anova_table.at['Deviation', 'df'])\n",
    "\n",
    "    # F critical\n",
    "    alpha = 0.05\n",
    "    # Possible types \"right-tailed, left-tailed, two-tailed\"\n",
    "    tail_hypothesis_type = \"two-tailed\"\n",
    "    if tail_hypothesis_type == \"two-tailed\":\n",
    "        alpha /= 2\n",
    "    anova_table.at['Trend', 'F crit'] = stats.f.ppf(1 - alpha, anova_table.at['Trend', 'df'], anova_table.at['Deviation', 'df'])\n",
    "\n",
    "    # R-squared\n",
    "    anova_table.at['Trend', 'R_sq (%)'] = SSr / SSt * 100\n",
    "\n",
    "    # Add degree information\n",
    "    anova_table['Degree'] = degree\n",
    "\n",
    "    # Append the ANOVA table to the results list\n",
    "    anova_results.append(anova_table)\n",
    "\n",
    "# Concatenate all ANOVA tables into a single DataFrame\n",
    "final_anova_df = pd.concat(anova_results)\n",
    "\n",
    "# Final ANOVA Table\n",
    "print(final_anova_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance on increase in order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also define a function for these ANOVA tests,  here's an example to test significance of icnrease in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Calculate significance of the increase in order from degree 1 to 2 and 2 to 3\n",
    "def calculate_significance(anova_df, degrees):\n",
    "    \n",
    "    significance_results = []\n",
    "\n",
    "    for i in range(1, len(degrees)):\n",
    "        degree_current = degrees[i]\n",
    "        degree_previous = degrees[i - 1]\n",
    "\n",
    "        # Extract SS and df for current and previous degrees\n",
    "        SSr_current = anova_df.loc[(anova_df['Degree'] == degree_current) & (anova_df.index == 'Trend'), 'SS'].values[0]\n",
    "        SSr_previous = anova_df.loc[(anova_df['Degree'] == degree_previous) & (anova_df.index == 'Trend'), 'SS'].values[0]\n",
    "\n",
    "        df_current = anova_df.loc[(anova_df['Degree'] == degree_current) & (anova_df.index == 'Trend'), 'df'].values[0]\n",
    "        df_previous = anova_df.loc[(anova_df['Degree'] == degree_previous) & (anova_df.index == 'Trend'), 'df'].values[0]\n",
    "\n",
    "        # Calculate the increase in SS and df\n",
    "        SS_increase = SSr_current - SSr_previous\n",
    "        df_increase = df_current - df_previous\n",
    "\n",
    "        # Extract MS for deviation for the current degree\n",
    "        MS_deviation_current = anova_df.loc[(anova_df['Degree'] == degree_current) & (anova_df.index == 'Deviation'), 'MS'].values[0]\n",
    "\n",
    "        # Calculate F-statistic\n",
    "        F_increase = (SS_increase / df_increase) / MS_deviation_current\n",
    "\n",
    "        # Calculate p-value\n",
    "        p_value = 1 - stats.f.cdf(F_increase, df_increase, df_current)\n",
    "\n",
    "        # Append results\n",
    "        significance_results.append({\n",
    "            'From Degree': degree_previous,\n",
    "            'To Degree': degree_current,\n",
    "            'SS Increase': SS_increase,\n",
    "            'df Increase': df_increase,\n",
    "            'F Increase': F_increase,\n",
    "            'p-value': p_value\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(significance_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display the significance results\n",
    "degrees = [1, 2, 3]\n",
    "\n",
    "significance_df = calculate_significance(final_anova_df, degrees)\n",
    "print(significance_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geostatspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
