{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was compiled for the course 'Geostatistics' at Ghent University (lecturer-in-charge: Prof. Dr. Ellen Van De Vijver; teaching assistant: Pablo De Weerdt). It consists of notebook snippets created by Michael Pyrcz. The code and markdown (text) snippets were edited specifically for this course, using the 'Jura data set' (Goovaerts, 1997) as example in the practical classes. Some new code snippets are also included to cover topics which were not found in the Geostastpy package demo books.\n",
    "\n",
    "This notebook is for educational purposes.<br> \n",
    "\n",
    "Guidelines for getting started were adapted from the 'Environmental Soil Sensing' course at Ghent University (lecturer-in-charge: Prof. Dr. Philippe De Smedt).<br> \n",
    "\n",
    "The Jura data set was taken from: Goovaerts P., 1997. Geostatistics for Natural Resources Evaluation. Oxford University Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't forget to save a copy on your Google drive before starting**\n",
    "\n",
    "You can also 'mount' your Google Drive in Google colab to directly access your Drive folders (e.g. to access data, previous notebooks etc.)\n",
    "\n",
    "Do not hesitate to contact us for questions or feel free to ask questions during the practical sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geostatistics: Introduction to geostatistical data analysis with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages for setup\n",
    "# -------------------------------------------- #\n",
    "\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clone the repository and add it to the path\n",
    "if 'google.colab' in sys.modules:\n",
    "    !git clone https://github.com/SENSE-UGent/E_I002454_Geostatistics.git\n",
    "    sys.path.append('/content/E_I002454_Geostatistics') #Default location in Google Colab after cloning\n",
    "else:\n",
    "    # if you are not using Google Colab, change the path to the location of the repository\n",
    "    sys.path.append(r'c:\\Users\\pdweerdt\\Documents\\Repos\\E_I002454_Geostatistics')\n",
    "\n",
    "# Import the setup function\n",
    "from Utils.setup import check_and_install_packages\n",
    "\n",
    "# Read the requirements.txt file\n",
    "if 'google.colab' in sys.modules:\n",
    "    requirements_path = '/content/E_I002454_Geostatistics/Utils/requirements.txt'\n",
    "else:\n",
    "    requirements_path = 'c:/Users/pdweerdt/Documents/Repos/E_I002454_Geostatistics/Utils/requirements.txt'\n",
    "\n",
    "with open(requirements_path) as f:\n",
    "    required_packages = f.read().splitlines()\n",
    "\n",
    "# Check and install packages\n",
    "check_and_install_packages(required_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geostatspy\n",
    "import geostatspy.GSLIB as GSLIB                              # GSLIB utilities, visualization and wrapper\n",
    "import geostatspy.geostats as geostats                        # if this raises an error, you might have to check your numba isntallation   \n",
    "print('GeostatsPy version: ' + str(geostatspy.__version__))   # these notebooks were tested with GeostatsPy version: 0.0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there was a small bug in the original kb2d_locations code from the geostatspy package\n",
    "# we have fixed this bug in the Utils.func module\n",
    "\n",
    "from Utils.func import kb2d_locations_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need some standard packages. These should have been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm                                         # suppress the status bar\n",
    "from functools import partialmethod\n",
    "\n",
    "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n",
    "                                   \n",
    "import numpy as np                                            # ndarrays for gridded data\n",
    "                                       \n",
    "import pandas as pd                                           # DataFrames for tabular data\n",
    "\n",
    "import matplotlib.pyplot as plt                               # for plotting\n",
    "\n",
    "from scipy import stats                                       # summary statistics\n",
    "\n",
    "plt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements\n",
    "\n",
    "ignore_warnings = True                                        # ignore warnings?\n",
    "if ignore_warnings == True:                                   \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.utils import io                                  # mute output from simulation\n",
    "\n",
    "seed = 42                                                     # random number seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are not required to run the given version of this practical exercise, but might be useful if you want to extend this notebook with more code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import math library\n",
    "import math\n",
    "\n",
    "import cmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr                              # Pearson product moment correlation\n",
    "from scipy.stats import spearmanr                             # spearman rank correlation    \n",
    "                                   \n",
    "import seaborn as sns                                         # advanced plotting\n",
    "\n",
    "import matplotlib as mpl                                        \n",
    "\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\n",
    "from matplotlib.colors import ListedColormap \n",
    "import matplotlib.ticker as mtick \n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Working Directory\n",
    "\n",
    "Do this to simplify subsequent reads and writes (avoid including the full address each time). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For use in Google Colab\n",
    "\n",
    "Run the following cell if you automatically want to get the data from the repository and store it on your Google Colab drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the working directory to the cloned repository\n",
    "\n",
    "os.chdir('E_I002454_Geostatistics')\n",
    "\n",
    "# get the current directory and store it as a variable\n",
    "\n",
    "cd = os.getcwd()\n",
    "print(\"Current Working Directory is \" , cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For local use\n",
    "\n",
    "Only run the following cell if you have the data locally stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the working directory, place an r in front to address special characters\n",
    "os.chdir(r'C:\\\\Users\\\\pdweerdt\\\\OneDrive - UGent\\\\I002454 - Geostatistics\\\\AY 2024-2025\\\\Practicals')\n",
    "\n",
    "# get the current directory and store it as a variable\n",
    "\n",
    "cd = os.getcwd()\n",
    "print(\"Current Working Directory is \" , cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tabular & Gridded Data\n",
    "\n",
    "Here's the section to load our data file into a Pandas' DataFrame object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and visualize a grid also.\n",
    "\n",
    "Check the datatype of your gridded data.\n",
    "\n",
    "In this case it is actually also a .dat file, so we can use the same function to import it. The .grid extension was given to indicate that it is gridded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can adjust the relative Path to the data folder\n",
    "\n",
    "data_path = cd + '/Hard_data' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '//prediction.dat'\n",
    "\n",
    "df = GSLIB.GSLIB2Dataframe(data_path + file_name) # read the data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_file_name = '//rocktype.grid'\n",
    "\n",
    "# load the data\n",
    "\n",
    "df_grid = GSLIB.GSLIB2Dataframe(data_path + grid_file_name)\n",
    "\n",
    "df_grid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Cd'\n",
    "unit = 'ppm'\n",
    "dist_unit = 'km'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define a colormap\n",
    "\n",
    "cmap = plt.cm.inferno                                         # color map inferno\n",
    "\n",
    "cmap_rainb = plt.cm.turbo # similar to what is shown on the slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In P1 we calculated some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_feat = round((df[feature].values).min(), 2)                    # calculate the minimum\n",
    "max_feat = round((df[feature].values).max(), 2)                    # calculate the maximum\n",
    "mean_feat = round((df[feature].values).mean(), 2)                  # calculate the mean\n",
    "stdev_feat = round((df[feature].values).std(), 2)                  # calculate the standard deviation\n",
    "n_feat = df[feature].values.size                                   # calculate the number of data\n",
    "\n",
    "print('The minimum is ' + str(min_feat) + ' ' + str(unit) + '.')   # print univariate statistics\n",
    "print('The maximum is ' + str(max_feat) + ' ' + str(unit) + '.')\n",
    "print('The mean is ' + str(mean_feat) + ' ' + str(unit) + '.')\n",
    "print('The standard deviation is ' + str(stdev_feat) + ' ' + str(unit) + '.')\n",
    "print('The number of data is ' + str(n_feat) + '.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinary Kriging for prediction Maps\n",
    "\n",
    "Remember that we use both our prediction data and a variogram model as inputs into the OK prediction algorithm. We already loaded our prediction data and grid. Let's have a look at the grid and also define our search neigbourhood and the variogram model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can initialise a new column into our grid dataframe for the OK prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grid[feature + 'OK'] = -99999 # assign a dummy value to the new feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin = 0; xmax = np.ceil(df.Xloc.max()) # range of x values\n",
    "ymin = 0; ymax = np.ceil(df.Yloc.max()) # range of y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSLIB.locmap_st(df_grid,'x', 'y', feature + 'OK',\n",
    "                0, 5.2, ymin, ymax, \n",
    "                1, 5, # set the value range for the color map\n",
    "                (\n",
    "                    'Location Map Grid points ' \n",
    "               #   + str(grid_feature)\n",
    "                 ), \n",
    "                 'X (km)', 'Y (km)',\n",
    "             (\n",
    "               #   str(grid_feature) + ' (' + str(unit) + ')'\n",
    "                 ), 'gray')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can hardly differentiate the individual grid points but they are surely there! Also ignore the colobar in this case as we focus on the grid locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variogram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nug = 0.3; nst = 1 # 2 nest structure variogram model parameters\n",
    "it1 = 2;            # 1=spherical, 2=exponential, 3=gaussian\n",
    "cc1 = 0.6; \n",
    "azi1 = 45; \n",
    "hmaj1 = 0.68; hmin1 = 0.68\n",
    "\n",
    "if nst==2:\n",
    "\n",
    "    it2 = 2; # prefereably same as it1\n",
    "    cc2 = 4.2; # sill contribution of the second structure in major direction\n",
    "    azi2 = 45; # direction with maximum spatial continuity (perpendicular to the major axis)\n",
    "    hmaj2 = 1000; hmin2 = 1.1\n",
    "\n",
    "else:\n",
    "\n",
    "    it2= np.nan\n",
    "    cc2= np.nan\n",
    "    azi2= np.nan\n",
    "    hmaj2= np.nan\n",
    "    hmin2= np.nan\n",
    "\n",
    "vario_mod = GSLIB.make_variogram(nug,nst,\n",
    "                                it1,cc1,azi1,hmaj1,hmin1,\n",
    "                                it2,cc2,azi2,hmaj2,hmin2\n",
    "                                ) # make model object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_points = 15\n",
    "min_points = 2\n",
    "search_radii = [1, 1]   # search radius for neighbouring data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try ordinary kriging\n",
    "\n",
    "* to switch to ordinary kriging set the kriging ktype to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display    \n",
    "\n",
    "tmin = -999; tmax = 9999\n",
    "\n",
    "ktype = 1   # ordinary kriging\n",
    "OK_kmap, OK_vmap = kb2d_locations_v2(df,\"Xloc\", \"Yloc\",feature,\n",
    "                                        tmin, tmax, \n",
    "                                        df_grid, 'x', 'y',\n",
    "                                        min_points, max_points, search_radii[0],\n",
    "                                        ktype, None, vario_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the OK_kmap to the df_grid\n",
    "df_grid[feature + 'OK'] = OK_kmap\n",
    "\n",
    "# add tjhe OK_vmap to the df_grid\n",
    "df_grid[feature + 'OK_var'] = OK_vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cmap_rainb = plt.cm.turbo # similar to what is shown on the slides\n",
    "\n",
    "plt.subplot(121)\n",
    "GSLIB.locmap_st(df_grid,'x', 'y', feature + 'OK',\n",
    "                0, 5.2, ymin, ymax, \n",
    "                0, 3, # set the value range for the color map\n",
    "                ('Location Map ' + str(feature + 'OK ')), 'X (km)', 'Y (km)',\n",
    "             (str(feature) + '_OK (' + str(unit) + ')'), cmap_rainb)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.1, hspace=0.2)\n",
    "\n",
    "plt.subplot(122)\n",
    "GSLIB.locmap_st(df_grid,'x', 'y', feature + 'OK_var',\n",
    "                0, 5.2, ymin, ymax, \n",
    "                0, 3, # set the value range for the color map\n",
    "                ('Location Map ' + str(feature + ' OK_var')), 'X (km)', 'Y (km)',\n",
    "             (str(feature) + '_OK_var ()' + unit + '$^2$)' + ')'), cmap_rainb)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.0, wspace=0.3, hspace=0.3); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jackknife validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the process but choose validation locations as the grid where you want to make predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '//validation.dat'\n",
    "\n",
    "df_val = GSLIB.GSLIB2Dataframe(data_path + file_name) # read the data\n",
    "\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display  \n",
    "\n",
    "val_method = 'jk'\n",
    "\n",
    "max_points = 15\n",
    "min_points = 2\n",
    "search_radii = [1,1]\n",
    "\n",
    "feature = 'Cd'\n",
    "\n",
    "# Initialize empty lists to add to the results df\n",
    "val_method_vals = []\n",
    "\n",
    "MPE_vals = []\n",
    "MSPE_vals = []\n",
    "RMSPE_vals = []\n",
    "MAPE_vals = []\n",
    "rel_nna_vals = []\n",
    "Pr_vals = []\n",
    "Sr_vals = []\n",
    "\n",
    "results_df_v = pd.DataFrame()\n",
    "\n",
    "# Perform validation initialize variables\n",
    "\n",
    "a_c = 0 # for the cumulative error\n",
    "a = 0 # for the error\n",
    "a_c_a = 0 #for the absolute cum error\n",
    "a_c_s = 0 #for the squared cum error\n",
    "\n",
    "data_pred = df.copy()\n",
    "data_val = df_val.copy()\n",
    "\n",
    "# Perform OK\n",
    "tmin = -999; tmax = 9999\n",
    "\n",
    "ktype = 1   # ordinary kriging\n",
    "OK_kmap, OK_vmap = kb2d_locations_v2(df,\"Xloc\", \"Yloc\",feature,\n",
    "                                        tmin, tmax, \n",
    "                                        data_val, 'Xloc', 'Yloc',\n",
    "                                        min_points, max_points, search_radii[0],\n",
    "                                        ktype, None, \n",
    "                                        vario_mod # As modelled earlier!\n",
    "                                        )\n",
    "\n",
    "data_val['OK' + feature] = OK_kmap\n",
    "data_val['OK' + feature + '_var'] = OK_vmap\n",
    "\n",
    "# Calculate error on test set\n",
    "data_val['r'] = data_val['OK' + feature] - data_val[feature] \n",
    "\n",
    "# print(\"r-value \", data_val['r'])\n",
    "\n",
    "data_val['r_s'] = data_val['r']**2\n",
    "\n",
    "data_val['r_a'] = data_val['r'].abs()\n",
    "\n",
    "# Calculate average error\n",
    "a = data_val['r'].mean()\n",
    "\n",
    "a_s = data_val['r_s'].mean()\n",
    "\n",
    "a_a = data_val['r_a'].mean()\n",
    "\n",
    "a_c = data_val['r'].sum() #cumulative error\n",
    "\n",
    "a_c_a = data_val['r_a'].sum() #cumulative absolute error\n",
    "\n",
    "a_c_s = data_val['r_s'].sum() #cumulative squared error\n",
    "\n",
    "# Round ac and aca\n",
    "a_c = round(a_c, 2)\n",
    "a_c_a = round(a_c_a, 2)\n",
    "a_c_s = round(a_c_s, 2)\n",
    "\n",
    "#calculate Mean prediction error\n",
    "MPE = round(a_c/n_feat, 2)\n",
    "\n",
    "print(\"Mean Prediction Error:\", MPE)    \n",
    "\n",
    "#Calculate Mean squared prediction error\n",
    "MSPE = round(a_c_s/n_feat, 2)\n",
    "print(\"Mean Squared Prediction Error:\", MSPE)\n",
    "\n",
    "#Calculate Root mean squared prediction error\n",
    "RMSPE = round(math.sqrt(a_c_s/n_feat), 2)\n",
    "print(\"Root Mean Squared Prediction Error:\", RMSPE)\n",
    "\n",
    "#calculate Mean absolute prediction error\n",
    "MAPE = round(a_c_a/n_feat, 2)\n",
    "print(\"Mean Absolute Prediction Error:\", MAPE)\n",
    "\n",
    "#Pearson correlation coefficient\n",
    "#read in the data, drop na to avoid errors\n",
    "data_cor = data_val.dropna(subset=[feature, 'OK' + feature])\n",
    "\n",
    "#extract the columns of interest \n",
    "x = data_cor[feature]\n",
    "y = data_cor['OK' + feature]\n",
    "\n",
    "#calculate the Pearson's correlation coefficient \n",
    "corr_p, _ = pearsonr(x, y)\n",
    "corr_p = round(corr_p, 2)\n",
    "print('Pearsons correlation: %.3f' % corr_p)\n",
    "\n",
    "# Spearman's Correlation:\n",
    "#calculate the Spearman's correlation coefficient \n",
    "corr_s, _ = spearmanr(x, y)\n",
    "corr_s = round(corr_s, 2)\n",
    "print('Spearmans correlation: %.3f' % corr_s)\n",
    "\n",
    "# Store the index values in the respective lists\n",
    "MPE_vals.append(MPE)\n",
    "MSPE_vals.append(MSPE)\n",
    "RMSPE_vals.append(RMSPE)\n",
    "MAPE_vals.append(MAPE)\n",
    "Pr_vals.append(corr_p)\n",
    "Sr_vals.append(corr_s)\n",
    "val_method_vals.append(val_method)\n",
    "\n",
    "# Create a new DataFrame to store the results for this variable and parameter settings\n",
    "results_temp_df = pd.DataFrame()\n",
    "results_temp_df['ValidationMethod'] = val_method_vals\n",
    "results_temp_df['MPE'] = MPE_vals\n",
    "results_temp_df['MSPE'] = MSPE_vals\n",
    "results_temp_df['RMSPE'] = RMSPE_vals\n",
    "results_temp_df['MAPE'] = MAPE_vals\n",
    "results_temp_df['PearsonCorr'] = Pr_vals\n",
    "results_temp_df['SpearmanCorr'] = Sr_vals\n",
    "\n",
    "# Append the results for this variable and parameter settings to the main DataFrame\n",
    "results_df_v_2d = pd.concat([results_df_v, results_temp_df], ignore_index=True)\n",
    "\n",
    "results_df_v_2d.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geostatspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
