{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disclaimer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notebook was compiled for the course 'Geostatistics' at Ghent University (lecturer-in-charge: Prof. Dr. Ellen Van De Vijver; teaching assistant: Pablo De Weerdt). It consists of notebook snippets created by Michael Pyrcz. The code and markdown (text) snippets were edited specifically for this course, using the 'Jura data set' (Goovaerts, 1997) as example in the practical classes. Some new code snippets are also included to cover topics which were not found in the Geostastpy package demo books.<br> <br>  **This is a draft notebook** The concepts are presented but actual methodology and results might differ from SGeMS outcomes.\n",
    "\n",
    "This notebook is for educational purposes.<br> \n",
    "\n",
    "Guidelines for getting started were adapted from the 'Environmental Soil Sensing' course at Ghent University (lecturer-in-charge: Prof. Dr. Philippe De Smedt).<br> \n",
    "\n",
    "The Jura data set was taken from: Goovaerts P., 1997. Geostatistics for Natural Resources Evaluation. Oxford University Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Don't forget to save a copy on your Google drive before starting**\n",
    "\n",
    "You can also 'mount' your Google Drive in Google colab to directly access your Drive folders (e.g. to access data, previous notebooks etc.)\n",
    "\n",
    "Do not hesitate to contact us for questions or feel free to ask questions during the practical sessions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geostatistics: Introduction to geostatistical data analysis with Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages for setup\n",
    "# -------------------------------------------- #\n",
    "\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Clone the repository and add it to the path\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "\n",
    "    repo_path = '/content/draft_E_I002454_Geostatistics'\n",
    "    if not os.path.exists(repo_path):\n",
    "        !git clone https://github.com/SENSE-UGent/E_I002454_Geostatistics.git\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.append(repo_path) #Default location in Google Colab after cloning\n",
    "\n",
    "else:\n",
    "    # if you are not using Google Colab, change the path to the location of the repository\n",
    "\n",
    "    repo_path = r'c:/Users/pdweerdt/Documents/Repos/draft_E_I002454_Geostatistics' # Change this to the location of the repository on your machine\n",
    "    if repo_path not in sys.path:\n",
    "        sys.path.append(repo_path) \n",
    "\n",
    "# Import the setup function\n",
    "from Utils.setup import check_and_install_packages\n",
    "\n",
    "# Read the requirements.txt file\n",
    "\n",
    "requirements_path = repo_path + '/Utils/requirements.txt'\n",
    "\n",
    "with open(requirements_path) as f:\n",
    "    required_packages = f.read().splitlines()\n",
    "\n",
    "# Check and install packages\n",
    "check_and_install_packages(required_packages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geostatspy\n",
    "import geostatspy.GSLIB as GSLIB                              # GSLIB utilities, visualization and wrapper\n",
    "import geostatspy.geostats as geostats                        # if this raises an error, you might have to check your numba installation   \n",
    "print('GeostatsPy version: ' + str(geostatspy.__version__))   # these notebooks were tested with GeostatsPy version: 0.0.72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Utils.func import (read_mod_file, beyond2, ik2d_v2_loc, ik2d_v2, ordrel2, \n",
    "    # cova2, \n",
    "    calculate_etype_and_conditional_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need some standard packages. These should have been installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm                                         # suppress the status bar\n",
    "from functools import partialmethod\n",
    "\n",
    "tqdm.__init__ = partialmethod(tqdm.__init__, disable=True)\n",
    "                                   \n",
    "import numpy as np                                            # ndarrays for gridded data\n",
    "                                       \n",
    "import pandas as pd                                           # DataFrames for tabular data\n",
    "\n",
    "import matplotlib.pyplot as plt                               # for plotting\n",
    "\n",
    "from scipy import stats                                       # summary statistics\n",
    "\n",
    "plt.rc('axes', axisbelow=True)                                # plot all grids below the plot elements\n",
    "\n",
    "ignore_warnings = True                                        # ignore warnings?\n",
    "if ignore_warnings == True:                                   \n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.utils import io                                  # mute output from simulation\n",
    "\n",
    "seed = 42                                                     # random number seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are not required to run the given version of this practical exercise, but might be useful if you want to extend this notebook with more code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import math library\n",
    "import math\n",
    "\n",
    "import cmath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr                              # Pearson product moment correlation\n",
    "from scipy.stats import spearmanr                             # spearman rank correlation    \n",
    "                                   \n",
    "import seaborn as sns                                         # advanced plotting\n",
    "\n",
    "import matplotlib as mpl                                        \n",
    "\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\n",
    "from matplotlib.colors import ListedColormap \n",
    "import matplotlib.ticker as mtick \n",
    "import matplotlib.gridspec as gridspec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the Working Directory\n",
    "\n",
    "Do this to simplify subsequent reads and writes (avoid including the full address each time). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For use in Google Colab\n",
    "\n",
    "Run the following cell if you automatically want to get the data from the repository and store it on your Google Colab drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current directory and store it as a variable\n",
    "\n",
    "cd = os.getcwd()\n",
    "print('Current Working Directory is ', cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For local use\n",
    "\n",
    "Only run the following cell if you have the data locally stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the working directory, place an r in front to address special characters\n",
    "os.chdir(r'c:\\Users\\pdweerdt\\Documents\\Repos')\n",
    "\n",
    "# get the current directory and store it as a variable\n",
    "\n",
    "cd = os.getcwd()\n",
    "print('Current Working Directory is ', cd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Tabular & Gridded Data\n",
    "\n",
    "Here's the section to load our data file into a Pandas' DataFrame object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and visualize a grid also.\n",
    "\n",
    "Check the datatype of your gridded data.\n",
    "\n",
    "In this case it is actually also a .dat file, so we can use the same function to import it. The .grid extension was given to indicate that it is gridded data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you can adjust the relative Path to the data folder\n",
    "\n",
    "data_path = cd + '/draft_E_I002454_Geostatistics/Hard_data' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can actually just import the prediction dataset but let's import the same data as used in SGeMS just to be sure. Note that actual thresholds can be slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '//Cd_9thresh.dat'\n",
    "\n",
    "df = GSLIB.GSLIB2Dataframe(data_path + file_name) # read the data\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_file_name = '//rocktype.grid'\n",
    "\n",
    "# load the data\n",
    "\n",
    "df_grid = GSLIB.GSLIB2Dataframe(data_path + grid_file_name)\n",
    "\n",
    "df_grid.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Cd'\n",
    "unit = 'ppm'\n",
    "dist_unit = 'km'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # grid plotting parameters\n",
    "xmin = 0; xmax = np.ceil(df.X.max()) # range of x values\n",
    "ymin = 0; ymax = np.ceil(df.Y.max()) # range of y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  define a colormap\n",
    "\n",
    "cmap = plt.cm.inferno                                         # color map inferno\n",
    "\n",
    "cmap_rainb = plt.cm.turbo # similar to what is shown on the slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate some statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In P1 we calculated some statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_feat = round((df[feature].values).min(), 2)                    # calculate the minimum\n",
    "max_feat = round((df[feature].values).max(), 2)                    # calculate the maximum\n",
    "mean_feat = round((df[feature].values).mean(), 2)                  # calculate the mean\n",
    "stdev_feat = round((df[feature].values).std(), 2)                  # calculate the standard deviation\n",
    "n_feat = df[feature].values.size                                   # calculate the number of data\n",
    "\n",
    "print('The minimum is ' + str(min_feat) + ' ' + str(unit) + '.')   # print univariate statistics\n",
    "print('The maximum is ' + str(max_feat) + ' ' + str(unit) + '.')\n",
    "print('The mean is ' + str(mean_feat) + ' ' + str(unit) + '.')\n",
    "print('The standard deviation is ' + str(stdev_feat) + ' ' + str(unit) + '.')\n",
    "print('The number of data is ' + str(n_feat) + '.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indicator Kriging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DRAFT VERSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indicator Kriging for continuous data\n",
    "\n",
    "To demonstrate indicator kriging variogram models are given, rather than calculate experimental variograms and then model them.\n",
    "\n",
    "Let's first set up the basic indicator kriging parameters:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's specify the thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  often the thresholds are chosen corresponding to the 9 deciles of the data\n",
    "#  \n",
    "#  the probabilities corresponding to the deciles \n",
    "probabilities = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9] \n",
    "\n",
    "#  the deciles are calculated using the numpy percentile function, \n",
    "# this function assumes percentages instead of fractions, so we multiply the probabilities by 100\n",
    "deciles = np.percentile(df[feature].values, [p * 100 for p in probabilities]) # 10 deciles\n",
    "\n",
    "print('The deciles are: ' + str(deciles))\n",
    "\n",
    "ncut = 9                                                     # number of thresholds\n",
    "\n",
    "thresholds = deciles.copy() # copy the deciles to the thresholds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variograms have been modelled before so we can read the parameters from the .mod files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read .mod file\n",
    "\n",
    "# read all mod files corresonding to the 9 thresholds\n",
    "# use a for loop to read the mod files and store them in a list\n",
    "#  the mod files are stored in the data_path folder, with the name 'Cd_9thresh_0.mod', 'Cd_9thresh_1.mod', etc.\n",
    "varios = []\n",
    "\n",
    "for i in range(0, ncut):\n",
    "\n",
    "    mod_file_name = '//variogramthr' + str(i+1) + '.mod' # name of the .mod file\n",
    "    print('Reading ' + data_path + mod_file_name)\n",
    "    var = read_mod_file(data_path + '//varmods_IK' + mod_file_name) # read the data\n",
    "    # append the variogram to the list\n",
    "    varios.append(var)\n",
    "\n",
    "varios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the IK function parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ik2d_v2_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is still a bit slow, might take around 2min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for IK\n",
    "ivtype = 1                                                    # variable type, 0 - categorical, 1 - continuous\n",
    "tmin = -999; tmax = 9999\n",
    "ndmin = 2; ndmax = 15                                         # minimum and maximum data for kriging \n",
    "radius = 1                                                  # maximum search distance\n",
    "ktype = 1                                                     # kriging type, 0 - simple, 1 - ordinary\n",
    "\n",
    "results = ik2d_v2_loc(df,'X','Y',feature,ivtype,ncut,\n",
    "                      thresholds,probabilities,tmin, tmax, df_grid, 'x', 'y', \n",
    "                      ndmin, ndmax, radius, ktype, varios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at the results\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot one of the results\n",
    "result = 'estimate_thresh_3'\n",
    "\n",
    "GSLIB.locmap_st(results, 'x', 'y', result, 0, 5.2, ymin, ymax, \n",
    "                0, 1, # set the value range for the color map\n",
    "                (\n",
    "                    'Location Map Grid points ' \n",
    "               #   + str(grid_feature)\n",
    "                 ), \n",
    "                 'X (km)', 'Y (km)',\n",
    "             'probability', cmap_rainb)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2, top=2.2, wspace=0.25, hspace=0.25); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all the results,\n",
    "\n",
    "for i in range(1, ncut):\n",
    "    plt.subplot(4,2,i) # 3 rows, 3 columns, i+1 is the index of the subplot\n",
    "    GSLIB.locmap_st(results,'x', 'y', 'estimate_thresh_' + str(i),\n",
    "                0, 5.2, ymin, ymax, \n",
    "                0, 1, # set the value range for the color map\n",
    "                (\n",
    "                    'Location Map Grid points ' \n",
    "               #   + str(grid_feature)\n",
    "                 ), \n",
    "                 'X (km)', 'Y (km)',\n",
    "             'probability', cmap_rainb)\n",
    "    \n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2, top=5.0, wspace=0.125, hspace=0.125); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDF reconstruction parameters (ccdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ccdf parameters\n",
    "\n",
    "zmin=0.0 # lower bound\n",
    "zmax=np.nan # unbounded\n",
    "\n",
    "ltail=2 # lower tail; 1=power, #2 power with par, 3= lin, 4=hyperbo\n",
    "ltpar=2.5 #omega\n",
    "middle=1    # Straight Linear Interpolation for middle values\n",
    "mpar=0      # not applicable for linear interpolation\n",
    "utail=4 #upper tail; 1=power, #2 power with par, 3= lin, 4=hyperbo\n",
    "utpar=1.5 # omega\n",
    "\n",
    "UNEST = -1.0 #default value for unestimated in context of postprocessing functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ccdf value estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(beyond2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's map the probability to exceed a threshold of 1.2 ppm\n",
    "\n",
    "zval = 1.2\n",
    "\n",
    "# initialise a column for the results\n",
    "results['prob_exc'] = np.nan\n",
    "\n",
    "n_pred = results.shape[0]\n",
    "\n",
    "ccut = thresholds\n",
    "\n",
    "# loop over the results and calculate the cdf value for each threshold\n",
    "# iterate over every row in results\n",
    "for i in range(n_pred):\n",
    "    cdfval = UNEST\n",
    "    # get the cdf values for the current row\n",
    "    ccdf = results.iloc[i, 2:11]\n",
    "    \n",
    "    cdfval = beyond2(1, len(ccut), ccut, ccdf, 9, thresholds, probabilities, \n",
    "                            zmin, zmax, ltail, ltpar, middle, mpar, utail, utpar, zval, cdfval)[1]\n",
    "    \n",
    "    results.at[i, 'prob_exc'] = 1-cdfval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the resulting probability map\n",
    "\n",
    "GSLIB.locmap_st(results, 'x', 'y', 'prob_exc',\n",
    "                0, 5.2, ymin, ymax, \n",
    "                0, 1, # set the value range for the color map\n",
    "                (\n",
    "                    'Porbability to exceed threshold of 1.2 ppm ' \n",
    "               #   + str(grid_feature)\n",
    "                 ), \n",
    "                 'X (km)', 'Y (km)',\n",
    "             'probability', cmap_rainb)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2, top=2.2, wspace=0.25, hspace=0.25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_etype_and_conditional_variance(ccdf, ccut, maxdis, zmin, zmax, ltail, ltpar, middle, mpar, utail, utpar):\n",
    "#     \"\"\"\n",
    "#     Calculate the e-type and conditional variance based on ccdf and ccut arrays using beyond2 for CCDF reconstruction.\n",
    "\n",
    "#     Parameters:\n",
    "#         ccdf (list of float): Cumulative distribution function values.\n",
    "#         ccut (list of float): Cutoff values corresponding to ccdf.\n",
    "#         maxdis (int): Maximum discretization for calculations.\n",
    "#         zmin (float): Minimum Z value.\n",
    "#         zmax (float): Maximum Z value.\n",
    "#         ltail (int): Option to handle values in the lower tail.\n",
    "#         ltpar (float): Parameter for the lower tail option.\n",
    "#         middle (int): Option to handle values in the middle.\n",
    "#         mpar (float): Parameter for the middle option.\n",
    "#         utail (int): Option to handle values in the upper tail.\n",
    "#         utpar (float): Parameter for the upper tail option.\n",
    "\n",
    "#     Returns:\n",
    "#         tuple: A tuple containing e-type (float) and conditional variance (float).\n",
    "#     \"\"\"\n",
    "#     dis = 1.0 / maxdis\n",
    "#     cdfval = -0.5 * dis\n",
    "#     etype = 0.0\n",
    "#     ecv = 0.0\n",
    "\n",
    "#     for _ in range(maxdis):\n",
    "#         cdfval += dis\n",
    "#         zval = -1.0\n",
    "\n",
    "#         # Use beyond2 for CCDF reconstruction\n",
    "#         zval = beyond2(1, len(ccut), ccut, ccdf, 0, [], [], zmin, zmax, ltail, ltpar, middle, mpar, utail, utpar, zval, cdfval)[0]\n",
    "\n",
    "#         etype += zval\n",
    "#         ecv += zval * zval\n",
    "\n",
    "#     etype /= maxdis\n",
    "#     ecv = max((ecv / maxdis - etype * etype), 0.0)\n",
    "\n",
    "#     return etype, ecv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-type estimate, conditional variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e-type and conditional variance calculation\n",
    "# let's do posprocessing for every predicted grid cell,\n",
    "# every grid cell prediction constists a list of predicted cdf values \n",
    "\n",
    "# get the number of prediction locations\n",
    "n_pred = results.shape[0]\n",
    "\n",
    "ccut = thresholds\n",
    "\n",
    "# initiate columns for e-type and conditional variance\n",
    "results['etype'] = np.nan\n",
    "results['ecv'] = np.nan\n",
    "\n",
    "# iterate over every row in results\n",
    "for i in range(n_pred):\n",
    "    cdfval = -1.0\n",
    "    zval = -1.0\n",
    "    # get the cdf values for the current row\n",
    "    ccdf = results.iloc[i, 2:11]\n",
    "    # calculate the e-type and conditional variance\n",
    "    etype, ecv = calculate_etype_and_conditional_variance(ccdf, ccut, 9, zmin, zmax, ltail, ltpar, middle, mpar, utail, utpar)\n",
    "    # store the results in the DataFrame\n",
    "    results.at[i, 'etype'] = etype\n",
    "    results.at[i, 'ecv'] = ecv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the etype \n",
    "\n",
    "GSLIB.locmap_st(results, 'x', 'y', 'etype', 0, 5.2, ymin, ymax,\n",
    "                0, 3, # set the value range for the color map\n",
    "                (\n",
    "                    'E-type prediction IK' \n",
    "               #   + str(grid_feature)\n",
    "                 ), \n",
    "                 'X (km)', 'Y (km)',\n",
    "             'e-type' + str(unit), cmap_rainb)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2, top=2.2, wspace=0.25, hspace=0.25);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the conditional variance\n",
    "\n",
    "GSLIB.locmap_st(results, 'x', 'y', 'ecv', 0, 5.2, ymin, ymax,\n",
    "                0, 8, # set the value range for the color map\n",
    "                (\n",
    "                    'Conditional Variance IK' \n",
    "               #   + str(grid_feature)\n",
    "                 ), \n",
    "                 'X (km)', 'Y (km)',\n",
    "             'ecv' + str(unit), cmap_rainb)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2, top=2.2, wspace=0.25, hspace=0.25);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Gaussian simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DRAFT VERSION**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now only implmented for regular grid simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(geostats.sgsim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure grid parameters\n",
    "# Determine grid dimensions from the grid data\n",
    "nx = len(df_grid['x'].unique())\n",
    "ny = len(df_grid['y'].unique())\n",
    "nz = 1  # 2D grid, so nz=1\n",
    "\n",
    "# Get grid origin and cell size\n",
    "xmn = df_grid['x'].min()\n",
    "ymn = df_grid['y'].min()\n",
    "zmn = 0.0  # 2D grid\n",
    "xsiz = 0.05\n",
    "ysiz = 0.05\n",
    "zsiz = 0.05  # Not used in 2D\n",
    "\n",
    "# Block discretization (for point kriging, set to 1)\n",
    "nxdis, nydis, nzdis = 1, 1, 1\n",
    "\n",
    "print(f\"Grid dimensions: {nx} x {ny}\")\n",
    "print(f\"Grid origin: ({xmn}, {ymn})\")\n",
    "print(f\"Grid cell size: {xsiz} x {ysiz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :param vcol: name of the variable column\n",
    "#     :param wcol: name of the weight column, if None assumes equal weighting\n",
    "#     :param ismooth: if True then use a reference distribution\n",
    "#     :param dfsmooth: pandas DataFrame required if reference distribution is used\n",
    "#     :param smcol: reference distribution property (required if reference\n",
    "#                   distribution is used)\n",
    "#     :param smwcol: reference distribution weight (required if reference\n",
    "#                    distribution is used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code is still kinda slow. Please note this can take longtime (depending on number of realizations) around 1min per realization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zmin = 0.00; zmax = np.nan                                  # feature min and max values \n",
    "\n",
    "nreal = 50                                                 # number of realizations\n",
    "ndmin = 0; ndmax = 15                                     # number of data for each kriging system\n",
    "vario = GSLIB.make_variogram(nug=0.4,nst=1,it1=2,cc1=0.6,azi1=0.0,hmaj1=1.1,hmin1=1.1)\n",
    "tmin = -999; tmax = 999\n",
    "\n",
    "sim_sk = geostats.sgsim(df,'X','Y',feature,wcol=-1,scol=-1,tmin=tmin,tmax=tmax,itrans=1,ismooth=0,dftrans=0,tcol=0,\n",
    "            twtcol=0,zmin=zmin,zmax=zmax,\n",
    "            ltail=ltail,ltpar=ltpar,utail=utail,utpar=utpar, # as defined earlier!\n",
    "            nsim=nreal,\n",
    "            nx=nx,xmn=xmn,xsiz=xsiz,ny=ny,ymn=ymn,ysiz=ysiz, # grid, as defined earlier!\n",
    "            seed=73073,\n",
    "            ndmin=ndmin,ndmax=ndmax,nodmax=20,mults=1,nmult=3,noct=-1,ktype=0,colocorr=0.0,sec_map=0,vario=vario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the first 4 realizations\n",
    "\n",
    "\n",
    "\n",
    "for isim in range(0,4):\n",
    "    plt.subplot(2,2,isim+1) # 2 rows, 2 columns, isim+1 is the index of the subplot\n",
    "    GSLIB.locpix_st(sim_sk[isim],xmin,xmax,ymin,ymax,xsiz,0,3,\n",
    "                    df,'X','Y',feature,\n",
    "                    'Sequential Gaussian Simulation w. Simple Kriging #' + str(isim+1),\n",
    "                    'X(m)','Y(m)', str(feature) + '(' + str(unit) + ')', cmap_rainb)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.1, wspace=0., hspace=0.1); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.2 # threshold for probability calculation\n",
    "\n",
    "prob12 = geostats.local_probability_exceedance(realizations = sim_sk, threshold = threshold) # local probability featu > 1.2\n",
    "\n",
    "\n",
    "GSLIB.locpix_st(prob12,xmin,xmax,ymin,ymax,xsiz,0.0,1.,df,'X','Y',\n",
    "                feature,'PostSIM - Probability Exceed ' + str(threshold) + str(unit),\n",
    "                'X(m)', 'Y(m)','Probability Exceedance',cmap_rainb)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.5, top=1., wspace=0.2, hspace=0.2); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e_type = geostats.local_expectation(sim_sk)             # local expectation map\n",
    "local_stdev = geostats.local_standard_deviation(sim_sk) # local standard deviation map\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "GSLIB.locpix_st(e_type,xmin,xmax,ymin,ymax,xsiz,\n",
    "                0.0,3, #min and max for the color map\n",
    "                df,'X','Y',feature,'PostSIM - e-type Model',\n",
    "                'X(m)','Y(m)', str(feature) + 'e-type',cmap_rainb)\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "GSLIB.locpix_st(local_stdev,xmin,xmax,ymin,ymax,xsiz,\n",
    "                0.0,3, #min and max for the color map\n",
    "                df,'X','Y',feature,'PostSIM - Local Standard Deviation Model',\n",
    "                'X(m)','Y(m)' ,str(feature) + 'Local Standard Deviation',cmap_rainb)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=2.5, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jackknife validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the process but choose validation locations as the grid where you want to make predictions...\n",
    "\n",
    "Note: you can validate the e-type predictions from IK and SGS\n",
    "\n",
    "**NOT IMPLEMENTED YET, DIY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '//validation.dat'\n",
    "\n",
    "df_val = GSLIB.GSLIB2Dataframe(data_path + file_name) # read the data\n",
    "\n",
    "df_val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display  \n",
    "\n",
    "val_method = 'jk'\n",
    "\n",
    "max_points = 15\n",
    "min_points = 2\n",
    "search_radii = [1,1]\n",
    "\n",
    "feature = 'Cd'\n",
    "\n",
    "n_feat = df_val[feature].values.size                                   # calculate the number of data\n",
    "\n",
    "# Initialize empty lists to add to the results df\n",
    "val_method_vals = []\n",
    "\n",
    "MPE_vals = []\n",
    "MSPE_vals = []\n",
    "RMSPE_vals = []\n",
    "MAPE_vals = []\n",
    "rel_nna_vals = []\n",
    "Pr_vals = []\n",
    "Sr_vals = []\n",
    "\n",
    "results_df_v = pd.DataFrame()\n",
    "\n",
    "# Perform validation initialize variables\n",
    "\n",
    "a_c = 0 # for the cumulative error\n",
    "a = 0 # for the error\n",
    "a_c_a = 0 #for the absolute cum error\n",
    "a_c_s = 0 #for the squared cum error\n",
    "\n",
    "data_pred = df.copy()\n",
    "data_val = df_val.copy()\n",
    "\n",
    "# Perform \n",
    "tmin = -999; tmax = 9999\n",
    "\n",
    "method = '' #define method: OK, IK, ...\n",
    "\n",
    "results_val = # method function with parameters here\n",
    "\n",
    "data_val[method + feature] = results_val\n",
    "data_val[method + feature + '_var'] = results_val_var # if applicable\n",
    "\n",
    "# Calculate error on test set\n",
    "data_val['r'] = data_val[method + feature] - data_val[feature] \n",
    "\n",
    "# calculate number of residuals without NA values\n",
    "data_val['r'] = data_val['r'].replace(-99999, np.nan) # replace dummy value with NaN\n",
    "n_feat = data_val['r'].count() # count the number of residuals without NA values\n",
    "\n",
    "# print(\"r-value \", data_val['r'])\n",
    "\n",
    "data_val['r_s'] = data_val['r']**2\n",
    "\n",
    "data_val['r_a'] = data_val['r'].abs()\n",
    "\n",
    "# Calculate cumulative error\n",
    "\n",
    "a_c = data_val['r'].sum() #cumulative error\n",
    "\n",
    "a_c_a = data_val['r_a'].sum() #cumulative absolute error\n",
    "\n",
    "a_c_s = data_val['r_s'].sum() #cumulative squared error\n",
    "\n",
    "# Round ac and aca\n",
    "a_c = round(a_c, 2)\n",
    "a_c_a = round(a_c_a, 2)\n",
    "a_c_s = round(a_c_s, 2)\n",
    "\n",
    "#calculate Mean prediction error\n",
    "MPE = round(a_c/n_feat, 2)\n",
    "\n",
    "print(\"Mean Prediction Error:\", MPE)    \n",
    "\n",
    "#Calculate Mean squared prediction error\n",
    "MSPE = round(a_c_s/n_feat, 2)\n",
    "print(\"Mean Squared Prediction Error:\", MSPE)\n",
    "\n",
    "#Calculate Root mean squared prediction error\n",
    "RMSPE = round(math.sqrt(a_c_s/n_feat), 2)\n",
    "print(\"Root Mean Squared Prediction Error:\", RMSPE)\n",
    "\n",
    "#calculate Mean absolute prediction error\n",
    "MAPE = round(a_c_a/n_feat, 2)\n",
    "print(\"Mean Absolute Prediction Error:\", MAPE)\n",
    "\n",
    "#Pearson correlation coefficient\n",
    "#read in the data, drop na to avoid errors\n",
    "data_cor = data_val.dropna(subset=[feature, method + feature])\n",
    "\n",
    "#extract the columns of interest \n",
    "x = data_cor[feature]\n",
    "y = data_cor[method + feature]\n",
    "\n",
    "#calculate the Pearson's correlation coefficient \n",
    "corr_p, _ = pearsonr(x, y)\n",
    "corr_p = round(corr_p, 2)\n",
    "print('Pearsons correlation: %.3f' % corr_p)\n",
    "\n",
    "# Spearman's Correlation:\n",
    "#calculate the Spearman's correlation coefficient \n",
    "corr_s, _ = spearmanr(x, y)\n",
    "corr_s = round(corr_s, 2)\n",
    "print('Spearmans correlation: %.3f' % corr_s)\n",
    "\n",
    "# Store the index values in the respective lists\n",
    "MPE_vals.append(MPE)\n",
    "MSPE_vals.append(MSPE)\n",
    "RMSPE_vals.append(RMSPE)\n",
    "MAPE_vals.append(MAPE)\n",
    "Pr_vals.append(corr_p)\n",
    "Sr_vals.append(corr_s)\n",
    "val_method_vals.append(val_method)\n",
    "\n",
    "# Create a new DataFrame to store the results for this variable and parameter settings\n",
    "results_temp_df = pd.DataFrame()\n",
    "results_temp_df['ValidationMethod'] = val_method_vals\n",
    "results_temp_df['MPE'] = MPE_vals\n",
    "results_temp_df['MSPE'] = MSPE_vals\n",
    "results_temp_df['RMSPE'] = RMSPE_vals\n",
    "results_temp_df['MAPE'] = MAPE_vals\n",
    "results_temp_df['PearsonCorr'] = Pr_vals\n",
    "results_temp_df['SpearmanCorr'] = Sr_vals\n",
    "\n",
    "# Append the results for this variable and parameter settings to the main DataFrame\n",
    "results_df_v_2d = pd.concat([results_df_v, results_temp_df], ignore_index=True)\n",
    "\n",
    "results_df_v_2d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Draft code for IK on a regular grid** under construction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular grid IK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mticker                           # custom colorpar ticks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nxdis = 1; nydis = 1                                          # block kriging discretizations, 1 for point kriging\n",
    "ndmin = 2; ndmax = 15                                         # minimum and maximum data for kriging \n",
    "radius = 1                                                  # maximum search distance\n",
    "ktype = 1                                                     # kriging type, 0 - simple, 1 - ordinary\n",
    "ivtype = 1                                                    # variable type, 0 - categorical, 1 - continuous\n",
    "tmin = -999; tmax = 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture --no-display   \n",
    "UNEST = -999.0\n",
    "\n",
    "no_trend = np.zeros((1,1))                                    # null ndarray not of correct size so ik2d will not trend\n",
    "ikmap = ik2d_v2(df,'X','Y',feature,ivtype,0,ncut,thresholds,probabilities,no_trend,tmin,tmax,nx,xmn,xsiz,ny,ymn,ysiz,\n",
    "                # nxdis,nydis,\n",
    "                ndmin,ndmax,radius,ktype,varios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def locpix_colormaps_st(array,xmin,xmax,ymin,ymax,step,vmin,vmax,df,xcol,ycol,vcol,title,xlabel,ylabel,vlabel_loc,vlabel,cmap_loc,cmap):\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.arange(xmin, xmax, step), np.arange(ymax, ymin, -1 * step)\n",
    "    )\n",
    "    cs = plt.imshow(array,interpolation = None,extent = [xmin,xmax,ymin,ymax], vmin = vmin, vmax = vmax,cmap = cmap)\n",
    "    plt.scatter(df[xcol],df[ycol],s=None,c=df[vcol],marker=None,cmap=cmap_loc,vmin=vmin,vmax=vmax,alpha=0.8,linewidths=0.8,\n",
    "        edgecolors=\"black\",)\n",
    "    plt.title(title); plt.xlabel(xlabel); plt.ylabel(ylabel); plt.xlim(xmin, xmax); plt.ylim(ymin, ymax)\n",
    "    cbar_loc = plt.colorbar(orientation=\"vertical\",pad=0.08,ticks=[0, 1],\n",
    "            format=mticker.FixedFormatter(['Shale','Sand'])); cbar_loc.set_label(vlabel_loc, rotation=270,labelpad=20)\n",
    "    cbar = plt.colorbar(cs,orientation=\"vertical\",pad=0.05); cbar.set_label(vlabel, rotation=270,labelpad=20)\n",
    "    return cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (0,ncut):\n",
    "    plt.subplot(3,3,i+1) # 3 rows, 3 columns, i+1 is the index of the subplot\n",
    "    GSLIB.pixelplt_st(ikmap[:,:,i],\n",
    "                  df_grid['x'].min(),df_grid['x'].max(),df_grid['y'].min(),df_grid['y'].max(), #we have to use the actual min and max values\n",
    "                  0.05,0,1,'IK','X (km)','Y (km)',\n",
    "                  (str(feature) + str(i)), cmap_rainb); plt.show()\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.0, top=1.1, wspace=0.2, hspace=0.1); \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNEST = -1.0\n",
    "zmin=0.0\n",
    "zmax=5\n",
    "ltail=2\n",
    "ltpar=2.5\n",
    "middle=1    # Straight Linear Interpolation:\n",
    "mpar=0\n",
    "utail=4 #1=power, #2 power with par, 3= lin, 4=hyperbo\n",
    "utpar=1.5 # omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do posprocessing for every predicted grid cell,\n",
    "# every grid cell prediction constists a list of predicted cdf values \n",
    "\n",
    "cdfval = 0.15\n",
    "zval = UNEST\n",
    "\n",
    "# make a copy of the grid to store the cdf values\n",
    "ikmap_zvals = np.zeros((ny,nx,1))\n",
    "\n",
    "# iterate over the grid cells and get the cdf values for each cell\n",
    "for i in range(0,ny):\n",
    "    for j in range(0,nx):\n",
    "        zval = -1\n",
    "        # print the grid cell value\n",
    "        # print(f\"Grid cell ({i}, {j}): {ikmap[i,j]}\")\n",
    "        zval = beyond2(ivtype,ncut,thresholds,ikmap[i,j],ncut,thresholds,probabilities,\n",
    "                0,zmax,ltail,ltpar,middle,mpar,utail,utpar,zval,cdfval)\n",
    "        # append the zval to the grid cell\n",
    "        ikmap_zvals[i,j] = zval\n",
    "        print(f\"Grid cell ({i}, {j}): zval={zval}\")\n",
    "\n",
    "\n",
    "\n",
    "# zval = beyond2(ivtype,ncut,thresholds,ikmap[3],ng,gcut,probabilities,0,zmax,ltail,ltpar,middle,mpar,utail,utpar,UNEST,0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  calculate e-type and conditional variance for every grid cell\n",
    "ikmap_etype = np.zeros((ny,nx,1))\n",
    "ikmap_ecv = np.zeros((ny,nx,1))\n",
    "\n",
    "for i in range(0,ny):\n",
    "    for j in range(0,nx):\n",
    "        # print the grid cell value\n",
    "        # print(f\"Grid cell ({i}, {j}): {ikmap[i,j]}\")\n",
    "        etype, ecv = calculate_etype_and_conditional_variance(ikmap[i,j], thresholds, 100, zmin, zmax, ltail, ltpar, middle, mpar, utail, utpar)\n",
    "        # append the zval to the grid cell\n",
    "        ikmap_etype[i,j] = etype\n",
    "        ikmap_ecv[i,j] = ecv\n",
    "        print(f\"Grid cell ({i}, {j}): etype={etype}, ecv={ecv}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the etype\n",
    "# get the min and max values of the grid\n",
    "# fill unest with nan first\n",
    "ikmap_etype[ikmap_etype == -1] = np.nan\n",
    "ikmap_ecv[ikmap_ecv == UNEST] = np.nan\n",
    "# get the min and max values of the grid\n",
    "vmin = np.nanmin(ikmap_etype[:,:,0])\n",
    "vmax = np.nanmax(ikmap_etype[:,:,0])\n",
    "\n",
    "print(vmin,vmax)\n",
    "plt.imshow(ikmap_etype[:,:,0],interpolation = None,extent = [xmin,xmax,ymin,ymax], vmin = 0, vmax = 3,\n",
    "           cmap = cmap_rainb)\n",
    "# add colorbar\n",
    "plt.colorbar(orientation=\"vertical\",pad=0.08,ticks=[vmin, vmax])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geostatspy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
